{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFal6ieX01n",
        "outputId": "510bfac1-eb35-412f-ba8b-4b3cbb33e6ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear, BatchNorm1d, ReLU, GELU, Sigmoid, SiLU, LeakyReLU\n",
        "import numpy as np\n",
        "from pytorch_tabnet import sparsemax\n",
        "# from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "# from pytorchtools import EarlyStopping\n",
        "from entmax import entmax_bisect\n",
        "from functools import partial\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX9NXHyaePzH",
        "outputId": "2084c1bc-ad1b-4b83-ac49-c688e5bf192b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  backers_count  \\\n",
            "0           1            403   \n",
            "1           2              2   \n",
            "2           3            406   \n",
            "3           4              1   \n",
            "4           5             67   \n",
            "\n",
            "                                               blurb  \\\n",
            "0  The true biography of the historical figure, w...   \n",
            "1  FAM is the new mobile app which combines event...   \n",
            "2  A graphic novel about two magical ladies in love.   \n",
            "3  We are publishing a magazine that focuses on t...   \n",
            "4  A dark and magical film set in a brothel, an u...   \n",
            "\n",
            "   converted_pledged_amount country country_displayable_name  \\\n",
            "0                     14740      US        the United States   \n",
            "1                        14      GB       the United Kingdom   \n",
            "2                     21799      US        the United States   \n",
            "3                        10      US        the United States   \n",
            "4                      8175      AU                Australia   \n",
            "\n",
            "           created_at currency current_currency            deadline  ...  \\\n",
            "0 2015-08-05 02:11:53      USD              USD 2015-09-14 04:19:27  ...   \n",
            "1 2018-06-07 19:53:22      GBP              USD 2018-08-18 15:43:54  ...   \n",
            "2 2016-09-06 08:33:56      USD              USD 2016-11-03 00:00:00  ...   \n",
            "3 2011-07-15 03:55:33      USD              USD 2011-10-04 17:04:28  ...   \n",
            "4 2016-08-03 09:49:12      AUD              USD 2016-09-22 01:30:48  ...   \n",
            "\n",
            "   location_name  location_country location_state  \\\n",
            "0    Minneapolis                US             MN   \n",
            "1         London                GB        England   \n",
            "2       New York                US             NY   \n",
            "3          Logan                US             UT   \n",
            "4         Sydney                AU            NSW   \n",
            "\n",
            "  days_diff_created_at_deadline  days_diff_state_changed_at_launched_at  \\\n",
            "0                            40                                      30   \n",
            "1                            71                                      60   \n",
            "2                            57                                      29   \n",
            "3                            81                                      30   \n",
            "4                            49                                      30   \n",
            "\n",
            "   name_len  blurb_len  success_percent success_probability  fail_probability  \n",
            "0        56        117         1.228386              100.00              0.00  \n",
            "1        30        135         0.000110                0.01             99.99  \n",
            "2        26         49         1.089950              100.00              0.00  \n",
            "3        25        135         0.002000                0.20             99.80  \n",
            "4        12        127         0.113883               11.39             88.61  \n",
            "\n",
            "[5 rows x 34 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_excel('cleaned_data.xlsx')\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p0cFMMSxZFBb"
      },
      "outputs": [],
      "source": [
        "def initialize_non_glu(module, input_dim, output_dim):\n",
        "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(4 * input_dim))\n",
        "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
        "    # torch.nn.init.zeros_(module.bias)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ddirs4EnZFlL"
      },
      "outputs": [],
      "source": [
        "def initialize_glu(module, input_dim, output_dim):\n",
        "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(input_dim))\n",
        "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
        "    # torch.nn.init.zeros_(module.bias)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MRmpVuvIZIHx"
      },
      "outputs": [],
      "source": [
        "class GBN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Ghost Batch Normalization\n",
        "    https://arxiv.org/abs/1705.08741\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
        "        super(GBN, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
        "\n",
        "    def forward(self, x):\n",
        "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
        "        res = [self.bn(x_) for x_ in chunks]\n",
        "\n",
        "        return torch.cat(res, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FFcZwAKGZKVk"
      },
      "outputs": [],
      "source": [
        "class TabNetEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        n_d=8,\n",
        "        n_a=8,\n",
        "        n_steps=3,\n",
        "        gamma=1.3,\n",
        "        n_independent=2,\n",
        "        n_shared=2,\n",
        "        epsilon=1e-15,\n",
        "        virtual_batch_size=128,\n",
        "        momentum=0.02,\n",
        "        mask_type=\"sparsemax\",\n",
        "        group_attention_matrix=None,\n",
        "    ):\n",
        "            super(TabNetEncoder, self).__init__()\n",
        "            self.input_dim = input_dim\n",
        "            self.output_dim = output_dim\n",
        "            self.is_multi_task = isinstance(output_dim, list)\n",
        "            self.n_d = n_d\n",
        "            self.n_a = n_a\n",
        "            self.n_steps = n_steps\n",
        "            self.gamma = gamma\n",
        "            self.epsilon = epsilon\n",
        "            self.n_independent = n_independent\n",
        "            self.n_shared = n_shared\n",
        "            self.virtual_batch_size = virtual_batch_size\n",
        "            self.mask_type = mask_type\n",
        "            self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
        "            self.group_attention_matrix = group_attention_matrix\n",
        "\n",
        "            if self.group_attention_matrix is None:\n",
        "                # no groups\n",
        "                self.group_attention_matrix = torch.eye(self.input_dim)\n",
        "                self.attention_dim = self.input_dim\n",
        "            else:\n",
        "                self.attention_dim = self.group_attention_matrix.shape[0]\n",
        "\n",
        "            if self.n_shared > 0:\n",
        "                shared_feat_transform = torch.nn.ModuleList()\n",
        "                for i in range(self.n_shared):\n",
        "                    if i == 0:\n",
        "                        shared_feat_transform.append(\n",
        "                            Linear(self.input_dim, 2 * (n_d + n_a), bias=False)\n",
        "                        )\n",
        "                    else:\n",
        "                        shared_feat_transform.append(\n",
        "                            Linear(n_d + n_a, 2 * (n_d + n_a), bias=False)\n",
        "                        )\n",
        "\n",
        "            else:\n",
        "                shared_feat_transform = None\n",
        "\n",
        "            self.initial_splitter = FeatTransformer(\n",
        "                self.input_dim,\n",
        "                n_d + n_a,\n",
        "                shared_feat_transform,\n",
        "                n_glu_independent=self.n_independent,\n",
        "                virtual_batch_size=self.virtual_batch_size,\n",
        "                momentum=momentum,\n",
        "            )\n",
        "\n",
        "            self.feat_transformers = torch.nn.ModuleList()\n",
        "            self.att_transformers = torch.nn.ModuleList()\n",
        "\n",
        "            for step in range(n_steps):\n",
        "                transformer = FeatTransformer(\n",
        "                    self.input_dim,\n",
        "                    n_d + n_a,\n",
        "                    shared_feat_transform,\n",
        "                    n_glu_independent=self.n_independent,\n",
        "                    virtual_batch_size=self.virtual_batch_size,\n",
        "                    momentum=momentum,\n",
        "                )\n",
        "                attention = AttentiveTransformer(\n",
        "                    n_a,\n",
        "                    self.attention_dim,\n",
        "                    group_matrix=group_attention_matrix,\n",
        "                    virtual_batch_size=self.virtual_batch_size,\n",
        "                    momentum=momentum,\n",
        "                    mask_type=self.mask_type,\n",
        "                )\n",
        "                self.feat_transformers.append(transformer)\n",
        "                self.att_transformers.append(attention)\n",
        "\n",
        "    def forward(self, x, prior=None):\n",
        "        x = self.initial_bn(x)\n",
        "\n",
        "        bs = x.shape[0]  # batch size\n",
        "        if prior is None:\n",
        "            prior = torch.ones((bs, self.attention_dim)).to(x.device)\n",
        "\n",
        "        M_loss = 0\n",
        "        att = self.initial_splitter(x)[:, self.n_d :]\n",
        "        steps_output = []\n",
        "        for step in range(self.n_steps):\n",
        "            M = self.att_transformers[step](prior, att)\n",
        "            M_loss += torch.mean(\n",
        "                torch.sum(torch.mul(M, torch.log(M + self.epsilon)), dim=1)\n",
        "            )\n",
        "            # update prior\n",
        "            prior = torch.mul(self.gamma - M, prior)\n",
        "            # output\n",
        "            M_feature_level = torch.matmul(M, self.group_attention_matrix)\n",
        "            masked_x = torch.mul(M_feature_level, x)\n",
        "            out = self.feat_transformers[step](masked_x)\n",
        "            d = ReLU()(out[:, : self.n_d])\n",
        "            steps_output.append(d)\n",
        "            # update attention\n",
        "            att = out[:, self.n_d :]\n",
        "\n",
        "        M_loss /= self.n_steps\n",
        "        return steps_output, M_loss\n",
        "\n",
        "    def forward_masks(self, x):\n",
        "        x = self.initial_bn(x)\n",
        "        bs = x.shape[0]  # batch size\n",
        "        prior = torch.ones((bs, self.attention_dim)).to(x.device)\n",
        "        M_explain = torch.zeros(x.shape).to(x.device)\n",
        "        att = self.initial_splitter(x)[:, self.n_d :]\n",
        "        masks = {}\n",
        "\n",
        "        for step in range(self.n_steps):\n",
        "            M = self.att_transformers[step](prior, att)\n",
        "            M_feature_level = torch.matmul(M, self.group_attention_matrix)\n",
        "            masks[step] = M_feature_level\n",
        "            # update prior\n",
        "            prior = torch.mul(self.gamma - M, prior)\n",
        "            # output\n",
        "            masked_x = torch.mul(M_feature_level, x)\n",
        "            out = self.feat_transformers[step](masked_x)\n",
        "            d = ReLU()(out[:, : self.n_d])\n",
        "            # explain\n",
        "            step_importance = torch.sum(d, dim=1)\n",
        "            M_explain += torch.mul(M_feature_level, step_importance.unsqueeze(dim=1))\n",
        "            # update attention\n",
        "            att = out[:, self.n_d :]\n",
        "\n",
        "        return M_explain, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "b8padxi5Z-CX"
      },
      "outputs": [],
      "source": [
        "# class TabNetDecoder(torch.nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         input_dim,\n",
        "#         n_d=8,\n",
        "#         n_steps=3,\n",
        "#         n_independent=1,\n",
        "#         n_shared=1,\n",
        "#         virtual_batch_size=128,\n",
        "#         momentum=0.02,\n",
        "#     ):\n",
        "#             super(TabNetDecoder, self).__init__()\n",
        "#             self.input_dim = input_dim\n",
        "#             self.n_d = n_d\n",
        "#             self.n_steps = n_steps\n",
        "#             self.n_independent = n_independent\n",
        "#             self.n_shared = n_shared\n",
        "#             self.virtual_batch_size = virtual_batch_size\n",
        "\n",
        "#             self.feat_transformers = torch.nn.ModuleList()\n",
        "\n",
        "#             if self.n_shared > 0:\n",
        "#                 shared_feat_transform = torch.nn.ModuleList()\n",
        "#                 for i in range(self.n_shared):\n",
        "#                     shared_feat_transform.append(Linear(n_d, 2 * n_d, bias=False))\n",
        "#             else:\n",
        "#                 shared_feat_transform = None\n",
        "\n",
        "#             for step in range(n_steps):\n",
        "#                 transformer = FeatTransformer(\n",
        "#                     n_d,\n",
        "#                     n_d,\n",
        "#                     shared_feat_transform,\n",
        "#                     n_glu_independent=self.n_independent,\n",
        "#                     virtual_batch_size=self.virtual_batch_size,\n",
        "#                     momentum=momentum,\n",
        "#                 )\n",
        "#                 self.feat_transformers.append(transformer)\n",
        "\n",
        "#             self.reconstruction_layer = Linear(n_d, self.input_dim, bias=False)\n",
        "#             initialize_non_glu(self.reconstruction_layer, n_d, self.input_dim)\n",
        "\n",
        "#     def forward(self, steps_output):\n",
        "#         res = 0\n",
        "#         for step_nb, step_output in enumerate(steps_output):\n",
        "#             x = self.feat_transformers[step_nb](step_output)\n",
        "#             res = torch.add(res, x)\n",
        "#         res = self.reconstruction_layer(res)\n",
        "#         return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hSGpnN13ax4Z"
      },
      "outputs": [],
      "source": [
        "# class TabNetPretraining(torch.nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         input_dim,\n",
        "#         pretraining_ratio=0.2,\n",
        "#         n_d=8,\n",
        "#         n_a=8,\n",
        "#         n_steps=3,\n",
        "#         gamma=1.3,\n",
        "#         cat_idxs=[],\n",
        "#         cat_dims=[],\n",
        "#         cat_emb_dim=1,\n",
        "#         n_independent=2,\n",
        "#         n_shared=2,\n",
        "#         epsilon=1e-15,\n",
        "#         virtual_batch_size=128,\n",
        "#         momentum=0.02,\n",
        "#         mask_type=\"sparsemax\",\n",
        "#         n_shared_decoder=1,\n",
        "#         n_indep_decoder=1,\n",
        "#         group_attention_matrix=None,\n",
        "#     ):\n",
        "#         super(TabNetPretraining, self).__init__()\n",
        "\n",
        "#         self.cat_idxs = cat_idxs or []\n",
        "#         self.cat_dims = cat_dims or []\n",
        "#         self.cat_emb_dim = cat_emb_dim\n",
        "\n",
        "#         self.input_dim = input_dim\n",
        "#         self.n_d = n_d\n",
        "#         self.n_a = n_a\n",
        "#         self.n_steps = n_steps\n",
        "#         self.gamma = gamma\n",
        "#         self.epsilon = epsilon\n",
        "#         self.n_independent = n_independent\n",
        "#         self.n_shared = n_shared\n",
        "#         self.mask_type = mask_type\n",
        "#         self.pretraining_ratio = pretraining_ratio\n",
        "#         self.n_shared_decoder = n_shared_decoder\n",
        "#         self.n_indep_decoder = n_indep_decoder\n",
        "\n",
        "#         if self.n_steps <= 0:\n",
        "#             raise ValueError(\"n_steps should be a positive integer.\")\n",
        "#         if self.n_independent == 0 and self.n_shared == 0:\n",
        "#             raise ValueError(\"n_shared and n_independent can't be both zero.\")\n",
        "\n",
        "#         self.virtual_batch_size = virtual_batch_size\n",
        "#         self.embedder = EmbeddingGenerator(input_dim,\n",
        "#                                            cat_dims,\n",
        "#                                            cat_idxs,\n",
        "#                                            cat_emb_dim,\n",
        "#                                            group_attention_matrix)\n",
        "#         self.post_embed_dim = self.embedder.post_embed_dim\n",
        "\n",
        "#         self.masker = RandomObfuscator(self.pretraining_ratio,\n",
        "#                                        group_matrix=self.embedder.embedding_group_matrix)\n",
        "#         self.encoder = TabNetEncoder(\n",
        "#             input_dim=self.post_embed_dim,\n",
        "#             output_dim=self.post_embed_dim,\n",
        "#             n_d=n_d,\n",
        "#             n_a=n_a,\n",
        "#             n_steps=n_steps,\n",
        "#             gamma=gamma,\n",
        "#             n_independent=n_independent,\n",
        "#             n_shared=n_shared,\n",
        "#             epsilon=epsilon,\n",
        "#             virtual_batch_size=virtual_batch_size,\n",
        "#             momentum=momentum,\n",
        "#             mask_type=mask_type,\n",
        "#             group_attention_matrix=self.embedder.embedding_group_matrix,\n",
        "#         )\n",
        "#         self.decoder = TabNetDecoder(\n",
        "#             self.post_embed_dim,\n",
        "#             n_d=n_d,\n",
        "#             n_steps=n_steps,\n",
        "#             n_independent=self.n_indep_decoder,\n",
        "#             n_shared=self.n_shared_decoder,\n",
        "#             virtual_batch_size=virtual_batch_size,\n",
        "#             momentum=momentum,\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         Returns: res, embedded_x, obf_vars\n",
        "#             res : output of reconstruction\n",
        "#             embedded_x : embedded input\n",
        "#             obf_vars : which variable where obfuscated\n",
        "#         \"\"\"\n",
        "#         embedded_x = self.embedder(x)\n",
        "#         if self.training:\n",
        "#             masked_x, obfuscated_groups, obfuscated_vars = self.masker(embedded_x)\n",
        "#             # set prior of encoder with obfuscated groups\n",
        "#             prior = 1 - obfuscated_groups\n",
        "#             steps_out, _ = self.encoder(masked_x, prior=prior)\n",
        "#             res = self.decoder(steps_out)\n",
        "#             return res, embedded_x, obfuscated_vars\n",
        "#         else:\n",
        "#             steps_out, _ = self.encoder(embedded_x)\n",
        "#             res = self.decoder(steps_out)\n",
        "#             return res, embedded_x, torch.ones(embedded_x.shape).to(x.device)\n",
        "\n",
        "#     def forward_masks(self, x):\n",
        "#         embedded_x = self.embedder(x)\n",
        "#         return self.encoder.forward_masks(embedded_x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9ipKK6Y5bAgr"
      },
      "outputs": [],
      "source": [
        "class TabNetNoEmbeddings(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        n_d=8,\n",
        "        n_a=8,\n",
        "        n_steps=3,\n",
        "        gamma=1.3,\n",
        "        n_independent=2,\n",
        "        n_shared=2,\n",
        "        epsilon=1e-15,\n",
        "        virtual_batch_size=128,\n",
        "        momentum=0.02,\n",
        "        mask_type=\"sparsemax\",\n",
        "        group_attention_matrix=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Defines main part of the TabNet network without the embedding layers.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Number of features\n",
        "        output_dim : int or list of int for multi task classification\n",
        "            Dimension of network output\n",
        "            examples : one for regression, 2 for binary classification etc...\n",
        "        n_d : int\n",
        "            Dimension of the prediction  layer (usually between 4 and 64)\n",
        "        n_a : int\n",
        "            Dimension of the attention  layer (usually between 4 and 64)\n",
        "        n_steps : int\n",
        "            Number of successive steps in the network (usually between 3 and 10)\n",
        "        gamma : float\n",
        "            Float above 1, scaling factor for attention updates (usually between 1.0 to 2.0)\n",
        "        n_independent : int\n",
        "            Number of independent GLU layer in each GLU block (default 2)\n",
        "        n_shared : int\n",
        "            Number of independent GLU layer in each GLU block (default 2)\n",
        "        epsilon : float\n",
        "            Avoid log(0), this should be kept very low\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
        "        mask_type : str\n",
        "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
        "        group_attention_matrix : torch matrix\n",
        "            Matrix of size (n_groups, input_dim), m_ij = importance within group i of feature j\n",
        "        \"\"\"\n",
        "        super(TabNetNoEmbeddings, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.is_multi_task = isinstance(output_dim, list)\n",
        "        self.n_d = n_d\n",
        "        self.n_a = n_a\n",
        "        self.n_steps = n_steps\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_independent = n_independent\n",
        "        self.n_shared = n_shared\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.mask_type = mask_type\n",
        "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
        "\n",
        "        self.encoder = TabNetEncoder(\n",
        "            input_dim=input_dim,\n",
        "            output_dim=output_dim,\n",
        "            n_d=n_d,\n",
        "            n_a=n_a,\n",
        "            n_steps=n_steps,\n",
        "            gamma=gamma,\n",
        "            n_independent=n_independent,\n",
        "            n_shared=n_shared,\n",
        "            epsilon=epsilon,\n",
        "            virtual_batch_size=virtual_batch_size,\n",
        "            momentum=momentum,\n",
        "            mask_type=mask_type,\n",
        "            group_attention_matrix=group_attention_matrix\n",
        "        )\n",
        "\n",
        "        if self.is_multi_task:\n",
        "            self.multi_task_mappings = torch.nn.ModuleList()\n",
        "            for task_dim in output_dim:\n",
        "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
        "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
        "                self.multi_task_mappings.append(task_mapping)\n",
        "        else:\n",
        "            self.final_mapping = Linear(n_d, output_dim, bias=False)\n",
        "            initialize_non_glu(self.final_mapping, n_d, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = 0\n",
        "        steps_output, M_loss = self.encoder(x)\n",
        "        res = torch.sum(torch.stack(steps_output, dim=0), dim=0)\n",
        "\n",
        "        if self.is_multi_task:\n",
        "            # Result will be in list format\n",
        "            out = []\n",
        "            for task_mapping in self.multi_task_mappings:\n",
        "                out.append(task_mapping(res))\n",
        "        else:\n",
        "            out = self.final_mapping(res)\n",
        "        return out, M_loss\n",
        "\n",
        "    def forward_masks(self, x):\n",
        "        return self.encoder.forward_masks(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Sprfvhb8bo_Y"
      },
      "outputs": [],
      "source": [
        "class TabNet(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        n_d=8,\n",
        "        n_a=8,\n",
        "        n_steps=10,\n",
        "        gamma=1.5,\n",
        "        cat_idxs=[],\n",
        "        cat_dims=[],\n",
        "        cat_emb_dim=1,\n",
        "        n_independent=2,\n",
        "        n_shared=2,\n",
        "        epsilon=1e-15,\n",
        "        virtual_batch_size=64,\n",
        "        momentum=0.02,\n",
        "        mask_type=\"sparsemax\",\n",
        "        group_attention_matrix=[],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Defines TabNet network\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Initial number of features\n",
        "        output_dim : int\n",
        "            Dimension of network output\n",
        "            examples : one for regression, 2 for binary classification etc...\n",
        "        n_d : int\n",
        "            Dimension of the prediction  layer (usually between 4 and 64)\n",
        "        n_a : int\n",
        "            Dimension of the attention  layer (usually between 4 and 64)\n",
        "        n_steps : int\n",
        "            Number of successive steps in the network (usually between 3 and 10)\n",
        "        gamma : float\n",
        "            Float above 1, scaling factor for attention updates (usually between 1.0 to 2.0)\n",
        "        cat_idxs : list of int\n",
        "            Index of each categorical column in the dataset\n",
        "        cat_dims : list of int\n",
        "            Number of categories in each categorical column\n",
        "        cat_emb_dim : int or list of int\n",
        "            Size of the embedding of categorical features\n",
        "            if int, all categorical features will have same embedding size\n",
        "            if list of int, every corresponding feature will have specific size\n",
        "        n_independent : int\n",
        "            Number of independent GLU layer in each GLU block (default 2)\n",
        "        n_shared : int\n",
        "            Number of independent GLU layer in each GLU block (default 2)\n",
        "        epsilon : float\n",
        "            Avoid log(0), this should be kept very low\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
        "        mask_type : str\n",
        "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
        "        group_attention_matrix : torch matrix\n",
        "            Matrix of size (n_groups, input_dim), m_ij = importance within group i of feature j\n",
        "        \"\"\"\n",
        "        super(TabNet, self).__init__()\n",
        "        self.cat_idxs = cat_idxs or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.cat_emb_dim = cat_emb_dim\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_d = n_d\n",
        "        self.n_a = n_a\n",
        "        self.n_steps = n_steps\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_independent = n_independent\n",
        "        self.n_shared = n_shared\n",
        "        self.mask_type = mask_type\n",
        "\n",
        "        if self.n_steps <= 0:\n",
        "            raise ValueError(\"n_steps should be a positive integer.\")\n",
        "        if self.n_independent == 0 and self.n_shared == 0:\n",
        "            raise ValueError(\"n_shared and n_independent can't be both zero.\")\n",
        "\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.embedder = EmbeddingGenerator(input_dim,\n",
        "                                           cat_dims,\n",
        "                                           cat_idxs,\n",
        "                                           cat_emb_dim,\n",
        "                                           group_attention_matrix)\n",
        "        self.post_embed_dim = self.embedder.post_embed_dim\n",
        "\n",
        "        self.tabnet = TabNetNoEmbeddings(\n",
        "            self.post_embed_dim,\n",
        "            output_dim,\n",
        "            n_d,\n",
        "            n_a,\n",
        "            n_steps,\n",
        "            gamma,\n",
        "            n_independent,\n",
        "            n_shared,\n",
        "            epsilon,\n",
        "            virtual_batch_size,\n",
        "            momentum,\n",
        "            mask_type,\n",
        "            self.embedder.embedding_group_matrix\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedder(x)\n",
        "        return self.tabnet(x)\n",
        "\n",
        "    def forward_masks(self, x):\n",
        "        x = self.embedder(x)\n",
        "        return self.tabnet.forward_masks(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rM6VA0jTcJ1s"
      },
      "outputs": [],
      "source": [
        "class AttentiveTransformer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        group_dim,\n",
        "        group_matrix,\n",
        "        virtual_batch_size=128,\n",
        "        momentum=0.02,\n",
        "        mask_type=\"sparsemax\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize an attention transformer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Input size\n",
        "        group_dim : int\n",
        "            Number of groups for features\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
        "        mask_type : str\n",
        "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
        "        \"\"\"\n",
        "        super(AttentiveTransformer, self).__init__()\n",
        "        self.fc = Linear(input_dim, group_dim, bias=False)\n",
        "        initialize_non_glu(self.fc, input_dim, group_dim)\n",
        "        self.bn = GBN(\n",
        "            group_dim, virtual_batch_size=virtual_batch_size, momentum=momentum\n",
        "        )\n",
        "\n",
        "        if mask_type == \"sparsemax\":\n",
        "            # Sparsemax\n",
        "            self.selector = sparsemax.Sparsemax(dim=-1)\n",
        "        elif mask_type == \"entmax15\":\n",
        "            # Entmax\n",
        "            self.selector = sparsemax.Entmax15(dim=-1)\n",
        "            #new below\n",
        "        elif mask_type == \"entmoid15\":\n",
        "            # Entmoid15\n",
        "            # self.selector = sparsemax.Entmoid15()\n",
        "            self.selector = sparsemax.Entmoid15.apply\n",
        "        elif mask_type == \"alpha11entmax\":\n",
        "            self.selector = partial(entmax_bisect, alpha=1.1, dim=-1)\n",
        "        elif mask_type == \"alpha12entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.2)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.2, dim=-1)\n",
        "        elif mask_type == \"alpha13entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.3)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.3, dim=-1)\n",
        "        elif mask_type == \"alpha14entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.4)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.4, dim=-1)\n",
        "        elif mask_type == \"alpha15entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.5)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.5, dim=-1)\n",
        "        elif mask_type == \"alpha16entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.6)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.6, dim=-1)\n",
        "        elif mask_type == \"alpha17entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.7)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.7, dim=-1)\n",
        "        elif mask_type == \"alpha18entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.8)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.8, dim=-1)\n",
        "        elif mask_type == \"alpha19entmax\":\n",
        "            # self.selector = sparsemax.EntmaxBisect(dim=-1, alpha=1.9)\n",
        "            self.selector = partial(entmax_bisect, alpha=1.9, dim=-1)\n",
        "            #new up here\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Please choose either sparsemax\" + \"or entmax15 as masktype\"\n",
        "            )\n",
        "\n",
        "    def forward(self, priors, processed_feat):\n",
        "        x = self.fc(processed_feat)\n",
        "        x = self.bn(x)\n",
        "        x = torch.mul(x, priors)\n",
        "        x = self.selector(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "92RDulMBcgxF"
      },
      "outputs": [],
      "source": [
        "class FeatTransformer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        shared_layers,\n",
        "        n_glu_independent,\n",
        "        virtual_batch_size=128,\n",
        "        momentum=0.02,\n",
        "    ):\n",
        "        super(FeatTransformer, self).__init__()\n",
        "        \"\"\"\n",
        "        Initialize a feature transformer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Input size\n",
        "        output_dim : int\n",
        "            Output_size\n",
        "        shared_layers : torch.nn.ModuleList\n",
        "            The shared block that should be common to every step\n",
        "        n_glu_independent : int\n",
        "            Number of independent GLU layers\n",
        "        virtual_batch_size : int\n",
        "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
        "        momentum : float\n",
        "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
        "        \"\"\"\n",
        "\n",
        "        params = {\n",
        "            \"n_glu\": n_glu_independent,\n",
        "            \"virtual_batch_size\": virtual_batch_size,\n",
        "            \"momentum\": momentum,\n",
        "        }\n",
        "\n",
        "        if shared_layers is None:\n",
        "            # no shared layers\n",
        "            self.shared = torch.nn.Identity()\n",
        "            is_first = True\n",
        "        else:\n",
        "            self.shared = GLU_Block(\n",
        "                input_dim,\n",
        "                output_dim,\n",
        "                first=True,\n",
        "                shared_layers=shared_layers,\n",
        "                n_glu=len(shared_layers),\n",
        "                virtual_batch_size=virtual_batch_size,\n",
        "                momentum=momentum,\n",
        "            )\n",
        "            is_first = False\n",
        "\n",
        "        if n_glu_independent == 0:\n",
        "            # no independent layers\n",
        "            self.specifics = torch.nn.Identity()\n",
        "        else:\n",
        "            spec_input_dim = input_dim if is_first else output_dim\n",
        "            self.specifics = GLU_Block(\n",
        "                spec_input_dim, output_dim, first=is_first, **params\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        x = self.specifics(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jDR-WGW7clIP"
      },
      "outputs": [],
      "source": [
        "class GLU_Block(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Independent GLU block, specific to each step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        n_glu=2,\n",
        "        first=False,\n",
        "        shared_layers=None,\n",
        "        virtual_batch_size=128,\n",
        "        momentum=0.02,\n",
        "    ):\n",
        "        super(GLU_Block, self).__init__()\n",
        "        self.first = first\n",
        "        self.shared_layers = shared_layers\n",
        "        self.n_glu = n_glu\n",
        "        self.glu_layers = torch.nn.ModuleList()\n",
        "\n",
        "        params = {\"virtual_batch_size\": virtual_batch_size, \"momentum\": momentum}\n",
        "\n",
        "        fc = shared_layers[0] if shared_layers else None\n",
        "        self.glu_layers.append(GLU_Layer(input_dim, output_dim, fc=fc, **params))\n",
        "        for glu_id in range(1, self.n_glu):\n",
        "            fc = shared_layers[glu_id] if shared_layers else None\n",
        "            self.glu_layers.append(GLU_Layer(output_dim, output_dim, fc=fc, **params))\n",
        "\n",
        "    def forward(self, x):\n",
        "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
        "        if self.first:  # the first layer of the block has no scale multiplication\n",
        "            x = self.glu_layers[0](x)\n",
        "            layers_left = range(1, self.n_glu)\n",
        "        else:\n",
        "            layers_left = range(self.n_glu)\n",
        "\n",
        "        for glu_id in layers_left:\n",
        "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
        "            x = x * scale\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sJqYFxUbcn_K"
      },
      "outputs": [],
      "source": [
        "class GLU_Layer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim, output_dim, fc=None, virtual_batch_size=128, momentum=0.02\n",
        "    ):\n",
        "        super(GLU_Layer, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        if fc:\n",
        "            self.fc = fc\n",
        "        else:\n",
        "            self.fc = Linear(input_dim, 2 * output_dim, bias=False)\n",
        "        initialize_glu(self.fc, input_dim, 2 * output_dim)\n",
        "\n",
        "        self.bn = GBN(\n",
        "            2 * output_dim, virtual_batch_size=virtual_batch_size, momentum=momentum\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        out = torch.mul(x[:, : self.output_dim], torch.sigmoid(x[:, self.output_dim :]))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kqX0qTYVcwBZ"
      },
      "outputs": [],
      "source": [
        "class EmbeddingGenerator(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Classical embeddings generator\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, cat_dims, cat_idxs, cat_emb_dims, group_matrix):\n",
        "        \"\"\"This is an embedding module for an entire set of features\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Number of features coming as input (number of columns)\n",
        "        cat_dims : list of int\n",
        "            Number of modalities for each categorial features\n",
        "            If the list is empty, no embeddings will be done\n",
        "        cat_idxs : list of int\n",
        "            Positional index for each categorical features in inputs\n",
        "        cat_emb_dim : list of int\n",
        "            Embedding dimension for each categorical features\n",
        "            If int, the same embedding dimension will be used for all categorical features\n",
        "        group_matrix : torch matrix\n",
        "            Original group matrix before embeddings\n",
        "        \"\"\"\n",
        "        super(EmbeddingGenerator, self).__init__()\n",
        "\n",
        "        if cat_dims == [] and cat_idxs == []:\n",
        "            self.skip_embedding = True\n",
        "            self.post_embed_dim = input_dim\n",
        "            # self.embedding_group_matrix = group_matrix.to(group_matrix.device)\n",
        "            ###################################\n",
        "            if group_matrix is not None:\n",
        "                self.embedding_group_matrix = group_matrix.to(group_matrix.device)\n",
        "            else:\n",
        "                self.embedding_group_matrix = torch.eye(input_dim)\n",
        "            ###################################\n",
        "            return\n",
        "        else:\n",
        "            self.skip_embedding = False\n",
        "\n",
        "        self.post_embed_dim = int(input_dim + np.sum(cat_emb_dims) - len(cat_emb_dims))\n",
        "\n",
        "        self.embeddings = torch.nn.ModuleList()\n",
        "\n",
        "        for cat_dim, emb_dim in zip(cat_dims, cat_emb_dims):\n",
        "            self.embeddings.append(torch.nn.Embedding(cat_dim, emb_dim))\n",
        "\n",
        "        # record continuous indices\n",
        "        self.continuous_idx = torch.ones(input_dim, dtype=torch.bool)\n",
        "        self.continuous_idx[cat_idxs] = 0\n",
        "\n",
        "        # update group matrix\n",
        "        n_groups = group_matrix.shape[0]\n",
        "        self.embedding_group_matrix = torch.empty((n_groups, self.post_embed_dim),\n",
        "                                                  device=group_matrix.device)\n",
        "        for group_idx in range(n_groups):\n",
        "            post_emb_idx = 0\n",
        "            cat_feat_counter = 0\n",
        "            for init_feat_idx in range(input_dim):\n",
        "                if self.continuous_idx[init_feat_idx] == 1:\n",
        "                    # this means that no embedding is applied to this column\n",
        "                    self.embedding_group_matrix[group_idx, post_emb_idx] = group_matrix[group_idx, init_feat_idx]  # noqa\n",
        "                    post_emb_idx += 1\n",
        "                else:\n",
        "                    # this is a categorical feature which creates multiple embeddings\n",
        "                    n_embeddings = cat_emb_dims[cat_feat_counter]\n",
        "                    self.embedding_group_matrix[group_idx, post_emb_idx:post_emb_idx+n_embeddings] = group_matrix[group_idx, init_feat_idx] / n_embeddings  # noqa\n",
        "                    post_emb_idx += n_embeddings\n",
        "                    cat_feat_counter += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply embeddings to inputs\n",
        "        Inputs should be (batch_size, input_dim)\n",
        "        Outputs will be of size (batch_size, self.post_embed_dim)\n",
        "        \"\"\"\n",
        "        if self.skip_embedding:\n",
        "            # no embeddings required\n",
        "            return x\n",
        "\n",
        "        cols = []\n",
        "        cat_feat_counter = 0\n",
        "        for feat_init_idx, is_continuous in enumerate(self.continuous_idx):\n",
        "            # Enumerate through continuous idx boolean mask to apply embeddings\n",
        "            if is_continuous:\n",
        "                cols.append(x[:, feat_init_idx].float().view(-1, 1))\n",
        "            else:\n",
        "                cols.append(\n",
        "                    self.embeddings[cat_feat_counter](x[:, feat_init_idx].long())\n",
        "                )\n",
        "                cat_feat_counter += 1\n",
        "        # concat\n",
        "        post_embeddings = torch.cat(cols, dim=1)\n",
        "        return post_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jKD3k1ITc0zY"
      },
      "outputs": [],
      "source": [
        "# class RandomObfuscator(torch.nn.Module):\n",
        "#     \"\"\"\n",
        "#     Create and applies obfuscation masks.\n",
        "#     The obfuscation is done at group level to match attention.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, pretraining_ratio, group_matrix):\n",
        "#         \"\"\"\n",
        "#         This create random obfuscation for self suppervised pretraining\n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         pretraining_ratio : float\n",
        "#             Ratio of feature to randomly discard for reconstruction\n",
        "\n",
        "#         \"\"\"\n",
        "#         super(RandomObfuscator, self).__init__()\n",
        "#         self.pretraining_ratio = pretraining_ratio\n",
        "#         # group matrix is set to boolean here to pass all posssible information\n",
        "#         self.group_matrix = (group_matrix > 0) + 0.\n",
        "#         self.num_groups = group_matrix.shape[0]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         Generate random obfuscation mask.\n",
        "\n",
        "#         Returns\n",
        "#         -------\n",
        "#         masked input and obfuscated variables.\n",
        "#         \"\"\"\n",
        "#         bs = x.shape[0]\n",
        "\n",
        "#         obfuscated_groups = torch.bernoulli(\n",
        "#             self.pretraining_ratio * torch.ones((bs, self.num_groups), device=x.device)\n",
        "#         )\n",
        "#         obfuscated_vars = torch.matmul(obfuscated_groups, self.group_matrix)\n",
        "#         masked_input = torch.mul(1 - obfuscated_vars, x)\n",
        "#         return masked_input, obfuscated_groups, obfuscated_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9cPoag9zFGwd"
      },
      "outputs": [],
      "source": [
        " # Define categorical and numerical features\n",
        "categorical = [\"category_name\", \"category_slug\", \"country\", \"currency\",\"spotlight\",\"staff_pick\"]\n",
        "numerical = ['backers_count', 'converted_pledged_amount', 'fx_rate', 'goal', 'pledged', 'static_usd_rate', 'usd_pledged','days_diff_created_at_deadline','days_diff_state_changed_at_launched_at','name_len', 'blurb_len']\n",
        "# valid_states = ['failed', 'successful']\n",
        "# state_mapping = {state: idx for idx, state in enumerate(valid_states)}\n",
        "# data[\"state\"] = data[\"state\"].map(state_mapping)\n",
        "\n",
        "SEED = 42\n",
        "# np.random.seed(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# random.seed(SEED)\n",
        "\n",
        "# Label encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le  # Store encoders for later decoding\n",
        "\n",
        "# Define targets\n",
        "target_class = \"state\"  # Classification target\n",
        "target_reg = \"success_probability\"  # Regression target (continuous value like pledged amount)\n",
        "\n",
        "# Apply StandardScaler to numerical features\n",
        "scaler = StandardScaler()\n",
        "data[numerical] = scaler.fit_transform(data[numerical])\n",
        "\n",
        "# Normalize the regression target values using MinMaxScaler\n",
        "reg_scaler = StandardScaler()\n",
        "data[target_reg] = reg_scaler.fit_transform(data[[target_reg]])\n",
        "\n",
        "\n",
        "# Split data into train/test sets\n",
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# In this example, we use all of the features (both categorical and numerical).\n",
        "feature_cols = categorical + numerical\n",
        "\n",
        "# Convert the features and target columns into tensors.\n",
        "X_train = torch.tensor(train_df[feature_cols].values, dtype=torch.float32)\n",
        "X_test = torch.tensor(test_df[feature_cols].values, dtype=torch.float32)\n",
        "\n",
        "# For classification, the target is a long (integer) type.\n",
        "y_train_class = torch.tensor(train_df[target_class].values, dtype=torch.long)\n",
        "y_test_class = torch.tensor(test_df[target_class].values, dtype=torch.long)\n",
        "\n",
        "# For regression, we convert the target to float.\n",
        "# Even though the values are 0 or 1, treating them as floats allows you to interpret the regression output as a rate.\n",
        "y_train_reg = torch.tensor(train_df[target_reg].values, dtype=torch.float32)\n",
        "y_test_reg = torch.tensor(test_df[target_reg].values, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(train_df[target_reg].mean(), train_df[target_reg].std())\n",
        "# print(test_df[target_reg].mean(), test_df[target_reg].std()) \n",
        "# print(label_encoders[\"category\"].classes_)\n",
        "# print(data[\"category\"].unique())  # \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqGJr-elNIwN",
        "outputId": "6bb98b74-40a0-4c3a-a7c2-41b3792a14fd"
      },
      "outputs": [],
      "source": [
        "# from prettytable import PrettyTable\n",
        "# columns_to_keep = categorical + numerical + [target_reg]+ [target_class]\n",
        "\n",
        "# # Drop columns not in columns_to_keep\n",
        "# data_keep = data[columns_to_keep]\n",
        "# # Create a pretty table\n",
        "# pt = PrettyTable()\n",
        "# pt.field_names = data_keep.columns  # Set column headers\n",
        "\n",
        "# # Add rows to the table\n",
        "# for row in data_keep.head(10).values:\n",
        "#     pt.add_row(row)\n",
        "\n",
        "# # Print the table\n",
        "# print(pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "14I-NnmHFQg-"
      },
      "outputs": [],
      "source": [
        "class MultiTaskTabNet(nn.Module):\n",
        "    def __init__(self, input_dim, n_classes, group_attention_matrix=None):\n",
        "        super(MultiTaskTabNet, self).__init__()\n",
        "\n",
        "        self.tabnet = TabNet(\n",
        "            input_dim=input_dim,\n",
        "            output_dim=64,          #  latent feature\n",
        "            n_d=4,\n",
        "            n_a=4,\n",
        "            n_steps=2,\n",
        "            gamma=1.5,\n",
        "            n_independent=2,\n",
        "            n_shared=2,\n",
        "            virtual_batch_size=64,\n",
        "            momentum=0.02,\n",
        "            mask_type=\"entmax15\",\n",
        "            group_attention_matrix=group_attention_matrix\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LayerNorm(32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "\n",
        "        # Regression head\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LayerNorm(32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared_features, _ = self.tabnet(x)\n",
        "        class_output = self.classification_head(shared_features)\n",
        "        reg_output = self.regression_head(shared_features)\n",
        "        return class_output, reg_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import optuna\n",
        "\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# import torch\n",
        "\n",
        "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "# y_train_class = torch.tensor(y_train_class, dtype=torch.long)\n",
        "\n",
        "# train_data = TensorDataset(X_train, y_train_class)\n",
        "# train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# def objective(trial):\n",
        "#     output_dim = trial.suggest_categorical(\"output_dim\", [128, 256, 512])\n",
        "#     n_d = trial.suggest_int(\"n_d\", 32, 128, step=8)\n",
        "#     n_a = trial.suggest_int(\"n_a\", 32, 128, step=8)\n",
        "#     n_steps = trial.suggest_int(\"n_steps\", 5, 7)\n",
        "#     gamma = trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.1)\n",
        "#     lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "#     mask_type = trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax15\", \"alpha11entmax\"])\n",
        "    \n",
        "#     class_hidden_size = trial.suggest_int(\"class_hidden_size\", 96, 256, step=32)\n",
        "#     class_dropout_rate = trial.suggest_float(\"class_dropout_rate\", 0.3, 0.7)\n",
        "    \n",
        "#     reg_hidden_size = trial.suggest_int(\"reg_hidden_size\", 48, 128, step=16)\n",
        "#     reg_dropout_rate = trial.suggest_float(\"reg_dropout_rate\", 0.3, 0.9)\n",
        "    \n",
        "#     model = MultiTaskTabNet(input_dim=X_train.shape[1], n_classes=len(valid_states))\n",
        "    \n",
        "#     model.classification_head = nn.Sequential(\n",
        "#         nn.Linear(128, class_hidden_size),\n",
        "#         nn.BatchNorm1d(class_hidden_size),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(class_dropout_rate),\n",
        "\n",
        "#         nn.Linear(class_hidden_size, class_hidden_size // 2),\n",
        "#         nn.BatchNorm1d(class_hidden_size // 2),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(class_dropout_rate),\n",
        "\n",
        "#         nn.Linear(class_hidden_size // 2, class_hidden_size // 4),\n",
        "#         nn.BatchNorm1d(class_hidden_size // 4),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(class_dropout_rate),\n",
        "\n",
        "#         nn.Linear(class_hidden_size // 4, class_hidden_size // 8),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(class_dropout_rate),\n",
        "\n",
        "#         nn.Linear(class_hidden_size // 8, len(valid_states))\n",
        "#     )\n",
        "    \n",
        "#     model.regression_head = nn.Sequential(\n",
        "#         nn.Linear(128, reg_hidden_size),\n",
        "#         nn.BatchNorm1d(reg_hidden_size),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(reg_dropout_rate),\n",
        "\n",
        "#         nn.Linear(reg_hidden_size, reg_hidden_size // 2),\n",
        "#         nn.BatchNorm1d(reg_hidden_size // 2),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(reg_dropout_rate),\n",
        "\n",
        "#         nn.Linear(reg_hidden_size // 2, reg_hidden_size // 4),\n",
        "#         nn.BatchNorm1d(reg_hidden_size // 4),\n",
        "#         nn.LeakyReLU(),\n",
        "#         nn.Dropout(reg_dropout_rate),\n",
        "\n",
        "#         nn.Linear(reg_hidden_size // 4, 1)\n",
        "#     )\n",
        "    \n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "#     loss_class = nn.CrossEntropyLoss()\n",
        "    \n",
        "#     #   (10 Epoch)\n",
        "#     for epoch in range(25):\n",
        "#         model.train()\n",
        "#         for X_batch, y_batch in train_loader:\n",
        "#             optimizer.zero_grad()\n",
        "#             class_pred, _ = model(X_batch)\n",
        "#             loss = loss_class(class_pred, y_batch)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "    \n",
        "#     # \n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         class_pred_test, _ = model(X_test)\n",
        "#         test_pred = torch.argmax(class_pred_test, dim=1)\n",
        "#         accuracy = accuracy_score(y_test_class.numpy(), test_pred.numpy())\n",
        "    \n",
        "#     return accuracy  #  Optuna \n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=200)  \n",
        "\n",
        "# best_params = study.best_params\n",
        "# print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# final_model = MultiTaskTabNet(\n",
        "#     input_dim=X_train.shape[1],\n",
        "#     n_classes=len(valid_states),\n",
        "#     # output_dim=best_params[\"output_dim\"],\n",
        "#     n_d=best_params[\"n_d\"],\n",
        "#     n_a=best_params[\"n_a\"],\n",
        "#     n_steps=best_params[\"n_steps\"],\n",
        "#     gamma=best_params[\"gamma\"],\n",
        "#     mask_type=best_params[\"mask_type\"],\n",
        "# )\n",
        "\n",
        "\n",
        "# final_optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
        "# loss_class = nn.CrossEntropyLoss()  #  loss function \n",
        "\n",
        "# for epoch in range(50):  \n",
        "#     final_model.train()\n",
        "#     for X_batch, y_batch in train_loader:\n",
        "#         final_optimizer.zero_grad()\n",
        "#         class_pred, _ = final_model(X_batch)\n",
        "#         loss = loss_class(class_pred, y_batch)\n",
        "#         loss.backward()\n",
        "#         final_optimizer.step()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         class_pred_test, _ = final_model(X_test)\n",
        "#         test_pred = torch.argmax(class_pred_test, dim=1)\n",
        "#         accuracy = accuracy_score(y_test_class.numpy(), test_pred.numpy())\n",
        "    \n",
        "#     print(f\"Epoch {epoch+1}/50 - Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Train Class Loss: 0.0055, Acc: 0.9222, Reg Loss: 0.0675, R2: 0.7264\n",
            "Epoch 1/100, Test Class Acc: 0.9979, Test Reg R2: 0.9530\n",
            "Epoch 2/100, Train Class Loss: 4.3656, Acc: 0.9965, Reg Loss: 2.0850, R2: 0.9350\n",
            "Epoch 2/100, Test Class Acc: 0.9997, Test Reg R2: 0.9601\n",
            "Epoch 3/100, Train Class Loss: 0.0005, Acc: 0.9988, Reg Loss: 0.0006, R2: 0.9399\n",
            "Epoch 3/100, Test Class Acc: 1.0000, Test Reg R2: 0.9611\n",
            "Epoch 4/100, Train Class Loss: 0.0025, Acc: 0.9993, Reg Loss: 0.0280, R2: 0.9415\n",
            "Epoch 4/100, Test Class Acc: 0.9998, Test Reg R2: 0.9593\n",
            "Epoch 5/100, Train Class Loss: 6.8488, Acc: 0.9994, Reg Loss: 4.4356, R2: 0.9411\n",
            "Epoch 5/100, Test Class Acc: 1.0000, Test Reg R2: 0.9585\n",
            "Epoch 6/100, Train Class Loss: 5.7777, Acc: 0.9996, Reg Loss: 4.3160, R2: 0.9426\n",
            "Epoch 6/100, Test Class Acc: 1.0000, Test Reg R2: 0.9609\n",
            "Epoch 7/100, Train Class Loss: 4.1449, Acc: 0.9994, Reg Loss: 1.9499, R2: 0.9442\n",
            "Epoch 7/100, Test Class Acc: 1.0000, Test Reg R2: 0.9607\n",
            "Epoch 8/100, Train Class Loss: 6.0946, Acc: 0.9995, Reg Loss: 4.0095, R2: 0.9440\n",
            "Epoch 8/100, Test Class Acc: 1.0000, Test Reg R2: 0.9618\n",
            "Epoch 9/100, Train Class Loss: 0.0004, Acc: 0.9998, Reg Loss: 0.3661, R2: 0.9471\n",
            "Epoch 9/100, Test Class Acc: 1.0000, Test Reg R2: 0.9580\n",
            "Epoch 10/100, Train Class Loss: 0.0002, Acc: 0.9998, Reg Loss: 0.0665, R2: 0.9503\n",
            "Epoch 10/100, Test Class Acc: 0.9998, Test Reg R2: 0.9040\n",
            "Epoch 11/100, Train Class Loss: 0.0000, Acc: 0.9999, Reg Loss: 0.0272, R2: 0.9547\n",
            "Epoch 11/100, Test Class Acc: 1.0000, Test Reg R2: 0.8718\n",
            "Epoch 12/100, Train Class Loss: 0.0000, Acc: 0.9998, Reg Loss: 0.0455, R2: 0.9569\n",
            "Epoch 12/100, Test Class Acc: 1.0000, Test Reg R2: 0.8697\n",
            "Epoch 13/100, Train Class Loss: 0.0001, Acc: 0.9999, Reg Loss: 0.0101, R2: 0.9577\n",
            "Epoch 13/100, Test Class Acc: 1.0000, Test Reg R2: 0.8206\n",
            "Epoch 14/100, Train Class Loss: 0.0001, Acc: 0.9998, Reg Loss: 0.0598, R2: 0.9582\n",
            "Epoch 14/100, Test Class Acc: 1.0000, Test Reg R2: 0.8351\n",
            "Epoch 15/100, Train Class Loss: 0.0000, Acc: 0.9999, Reg Loss: 0.0238, R2: 0.9603\n",
            "Epoch 15/100, Test Class Acc: 0.9998, Test Reg R2: 0.8134\n",
            "Epoch 16/100, Train Class Loss: 0.0001, Acc: 0.9997, Reg Loss: 0.0169, R2: 0.9607\n",
            "Epoch 16/100, Test Class Acc: 1.0000, Test Reg R2: 0.7498\n",
            "Epoch 17/100, Train Class Loss: 4.9195, Acc: 0.9997, Reg Loss: 2.2292, R2: 0.9607\n",
            "Epoch 17/100, Test Class Acc: 1.0000, Test Reg R2: 0.7686\n",
            "Epoch 18/100, Train Class Loss: 0.0004, Acc: 0.9999, Reg Loss: 0.0344, R2: 0.9620\n",
            "Epoch 18/100, Test Class Acc: 1.0000, Test Reg R2: 0.8485\n",
            "\n",
            " Early stopping at epoch 18. Best R: 0.9618\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import r2_score\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# #  model\n",
        "# input_dim = X_train.shape[1]\n",
        "# num_classes = 2\n",
        "\n",
        "# model = MultiTaskTabNet(input_dim, n_classes=num_classes, group_attention_matrix=torch.eye(input_dim))\n",
        "\n",
        "# #  optimizer  loss functions\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "# loss_class = nn.CrossEntropyLoss()  #  Classification\n",
        "# loss_reg = nn.MSELoss()  #  Regression\n",
        "\n",
        "# #  DataLoader  training  test\n",
        "# train_data = TensorDataset(X_train, y_train_class, y_train_reg)\n",
        "# test_data = TensorDataset(X_test, y_test_class, y_test_reg)\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "# test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
        "\n",
        "# #  loss  metric\n",
        "# train_loss_history = []\n",
        "# test_loss_history = []\n",
        "# train_mse_history = []\n",
        "# test_mse_history = []\n",
        "# train_r2_history = []\n",
        "# test_r2_history = []\n",
        "# test_accuracy_history = []\n",
        "# train_accuracy_history = []\n",
        "\n",
        "# # Training loop\n",
        "# epochs = 100\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "\n",
        "#     train_preds_reg = []\n",
        "#     train_targets_reg = []\n",
        "#     train_preds_class = []\n",
        "#     train_targets_class = []\n",
        "\n",
        "#     # Loop through batches of data from train loader\n",
        "#     for X_batch, y_batch_class, y_batch_reg in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Forward pass\n",
        "#         class_pred, reg_pred = model(X_batch)\n",
        "\n",
        "#         #  shape  forward pass\n",
        "#         # print(\"y_batch_reg shape:\", y_batch_reg.shape)\n",
        "#         # print(\"reg_pred shape after squeeze:\", reg_pred.squeeze().shape)\n",
        "\n",
        "#         #  loss\n",
        "#         loss1 = loss_class(class_pred, y_batch_class)\n",
        "#         loss2 = loss_reg(reg_pred.squeeze(), y_batch_reg)\n",
        "\n",
        "#         total_loss = loss1 + loss2\n",
        "\n",
        "#         # Backpropagation\n",
        "#         total_loss.backward()\n",
        "#         # Gradient Clipping\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "#         optimizer.step()\n",
        "\n",
        "#         #  batch \n",
        "#         train_preds_reg.append(reg_pred.detach().cpu().numpy())\n",
        "#         train_targets_reg.append(y_batch_reg.detach().cpu().numpy())\n",
        "\n",
        "#         train_preds_class.append(torch.argmax(class_pred, dim=1).detach().cpu().numpy())\n",
        "#         train_targets_class.append(y_batch_class.detach().cpu().numpy())\n",
        "\n",
        "#     #   List  numpy array \n",
        "#     train_preds_reg = np.concatenate(train_preds_reg, axis=0)\n",
        "#     train_targets_reg = np.concatenate(train_targets_reg, axis=0)\n",
        "\n",
        "#     train_preds_class = np.concatenate(train_preds_class, axis=0)\n",
        "#     train_targets_class = np.concatenate(train_targets_class, axis=0)\n",
        "\n",
        "#     train_r2 = r2_score(train_targets_reg, train_preds_reg.squeeze())  # R score\n",
        "#     train_accuracy = (train_preds_class == train_targets_class).mean()  # Accuracy\n",
        "\n",
        "#     #  loss\n",
        "#     train_loss_history.append(total_loss.item())\n",
        "#     train_mse_history.append(loss2.item())\n",
        "#     train_r2_history.append(train_r2)\n",
        "#     train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "#           f\"Train Classification Loss: {loss1.item():.4f}, Train Classification Accuracy: {train_accuracy:.4f}, \"\n",
        "#           f\"Train Regression Loss (MSE): {loss2.item():.4f}, Train Regression R: {train_r2:.4f}\")\n",
        "\n",
        "#     # Evaluation mode\n",
        "#     model.eval()\n",
        "#     test_preds_reg = []\n",
        "#     test_targets_reg = []\n",
        "#     test_preds_class = []\n",
        "#     test_targets_class = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for X_batch, y_batch_class, y_batch_reg in test_loader:\n",
        "#             class_pred_test, reg_pred_test = model(X_batch)\n",
        "\n",
        "#             #  loss  test\n",
        "#             test_loss_class = loss_class(class_pred_test, y_batch_class).item()\n",
        "#             test_loss_reg = loss_reg(reg_pred_test.squeeze(), y_batch_reg).item()\n",
        "\n",
        "#             test_loss = test_loss_class + test_loss_reg\n",
        "#             test_loss_history.append(test_loss)\n",
        "\n",
        "#             #  batch  test\n",
        "#             test_preds_reg.append(reg_pred_test.detach().cpu().numpy())\n",
        "#             test_targets_reg.append(y_batch_reg.detach().cpu().numpy())\n",
        "\n",
        "#             test_preds_class.append(torch.argmax(class_pred_test, dim=1).detach().cpu().numpy())\n",
        "#             test_targets_class.append(y_batch_class.detach().cpu().numpy())\n",
        "\n",
        "#     #   List  numpy array \n",
        "#     test_preds_reg = np.concatenate(test_preds_reg, axis=0)\n",
        "#     test_targets_reg = np.concatenate(test_targets_reg, axis=0)\n",
        "\n",
        "#     test_preds_class = np.concatenate(test_preds_class, axis=0)\n",
        "#     test_targets_class = np.concatenate(test_targets_class, axis=0)\n",
        "\n",
        "#     test_r2 = r2_score(test_targets_reg, test_preds_reg.squeeze())\n",
        "#     test_accuracy = (test_preds_class == test_targets_class).mean()\n",
        "\n",
        "#     test_r2_history.append(test_r2)\n",
        "#     test_mse_history.append(test_loss_reg)\n",
        "#     test_accuracy_history.append(test_accuracy)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "#           f\"Test Classification Loss: {test_loss_class:.4f}, Test Classification Accuracy: {test_accuracy:.4f}, \"\n",
        "#           f\"Test Regression Loss (MSE): {test_loss_reg:.4f}, Test Regression R: {test_r2:.4f}\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#  CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "#  model\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 2\n",
        "model = MultiTaskTabNet(input_dim, n_classes=num_classes, group_attention_matrix=torch.eye(input_dim)).to(device)\n",
        "\n",
        "#  optimizer  loss functions\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "loss_class = nn.CrossEntropyLoss()  #  Classification\n",
        "loss_reg = nn.MSELoss()  #  Regression\n",
        "\n",
        "#  DataLoader  training  test\n",
        "train_data = TensorDataset(X_train, y_train_class, y_train_reg)\n",
        "test_data = TensorDataset(X_test, y_test_class, y_test_reg)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "\n",
        "# Early stopping parameters\n",
        "best_test_r2 = -np.inf\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    train_preds_reg, train_targets_reg = [], []\n",
        "    train_preds_class, train_targets_class = [], []\n",
        "\n",
        "    for X_batch, y_batch_class, y_batch_reg in train_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch_class = y_batch_class.to(device)\n",
        "        y_batch_reg = y_batch_reg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        class_pred, reg_pred = model(X_batch)\n",
        "\n",
        "        loss1 = loss_class(class_pred, y_batch_class)\n",
        "        loss2 = loss_reg(reg_pred.squeeze(), y_batch_reg)\n",
        "        total_loss = loss1 + loss2\n",
        "\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_preds_reg.append(reg_pred.detach().cpu().numpy())\n",
        "        train_targets_reg.append(y_batch_reg.detach().cpu().numpy())\n",
        "        train_preds_class.append(torch.argmax(class_pred, dim=1).detach().cpu().numpy())\n",
        "        train_targets_class.append(y_batch_class.detach().cpu().numpy())\n",
        "\n",
        "    train_preds_reg = np.concatenate(train_preds_reg)\n",
        "    train_targets_reg = np.concatenate(train_targets_reg)\n",
        "    train_preds_class = np.concatenate(train_preds_class)\n",
        "    train_targets_class = np.concatenate(train_targets_class)\n",
        "\n",
        "    train_r2 = r2_score(train_targets_reg, train_preds_reg.squeeze())\n",
        "    train_accuracy = (train_preds_class == train_targets_class).mean()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "          f\"Train Class Loss: {loss1.item():.4f}, Acc: {train_accuracy:.4f}, \"\n",
        "          f\"Reg Loss: {loss2.item():.4f}, R2: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    test_preds_reg, test_targets_reg = [], []\n",
        "    test_preds_class, test_targets_class = [], []\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch_class, y_batch_reg in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch_class = y_batch_class.to(device)\n",
        "            y_batch_reg = y_batch_reg.to(device)\n",
        "\n",
        "            class_pred_test, reg_pred_test = model(X_batch)\n",
        "\n",
        "            test_preds_reg.append(reg_pred_test.detach().cpu().numpy())\n",
        "            test_targets_reg.append(y_batch_reg.detach().cpu().numpy())\n",
        "            test_preds_class.append(torch.argmax(class_pred_test, dim=1).detach().cpu().numpy())\n",
        "            test_targets_class.append(y_batch_class.detach().cpu().numpy())\n",
        "\n",
        "    test_preds_reg = np.concatenate(test_preds_reg)\n",
        "    test_targets_reg = np.concatenate(test_targets_reg)\n",
        "    test_preds_class = np.concatenate(test_preds_class)\n",
        "    test_targets_class = np.concatenate(test_targets_class)\n",
        "\n",
        "    test_r2 = r2_score(test_targets_reg, test_preds_reg.squeeze())\n",
        "    test_accuracy = (test_preds_class == test_targets_class).mean()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "          f\"Test Class Acc: {test_accuracy:.4f}, Test Reg R2: {test_r2:.4f}\")\n",
        "    \n",
        "        # === Early Stopping logic ===\n",
        "    if test_r2 > best_test_r2 + 1e-4:\n",
        "        best_test_r2 = test_r2\n",
        "        counter = 0\n",
        "        best_model_state = model.state_dict()  # Save best model\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"\\n Early stopping at epoch {epoch+1}. Best R: {best_test_r2:.4f}\")\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ec-2MOZFVcN",
        "outputId": "baf21d93-f5d1-4aa0-e317-8ba7e1e779e6"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from sklearn.metrics import r2_score\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# # Define model\n",
        "# input_dim = X_train.shape[1]\n",
        "# num_classes = len(valid_states)\n",
        "# print(input_dim)\n",
        "# # num_classes = 1\n",
        "\n",
        "# model = MultiTaskTabNet(input_dim, n_classes=num_classes, group_attention_matrix=torch.eye(input_dim))\n",
        "\n",
        "# # Define optimizer and loss functions\n",
        "# # optimizer = torch.optim.Adam(model.parameters(), lr=0.00001) # 0.01 \n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.9, weight_decay=0.01)\n",
        "# # optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "\n",
        "# # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "# loss_class = nn.CrossEntropyLoss()  # For classification\n",
        "# loss_reg = nn.MSELoss()  # Use Mean Squared Error (MSE) for regression\n",
        "\n",
        "# train_loss_history = []\n",
        "# test_loss_history = []\n",
        "# train_mse_history = []\n",
        "# test_mse_history = []\n",
        "# train_r2_history = []\n",
        "# test_r2_history = []\n",
        "# test_accuracy_history = []\n",
        "# train_accuracy_history = []\n",
        "\n",
        "\n",
        "# # early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "# # Training loop\n",
        "# epochs = 50\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     # Forward pass\n",
        "#     class_pred, reg_pred = model(X_train)\n",
        "\n",
        "#     # Compute losses\n",
        "#     loss1 = loss_class(class_pred, y_train_class)\n",
        "#     loss2 = loss_reg(reg_pred.squeeze(), y_train_reg)  # MSE loss for regression\n",
        "\n",
        "#     # Total loss\n",
        "#     total_loss = loss1 + loss2\n",
        "\n",
        "#     # Backpropagation\n",
        "#     total_loss.backward()\n",
        "#     optimizer.step()\n",
        "    \n",
        "#     # scheduler.step()  \n",
        "\n",
        "#     # Append train loss and MSE history\n",
        "#     train_loss_history.append(total_loss.item())\n",
        "\n",
        "#     # Calculate training MSE and R\n",
        "#     train_mse = loss2.item()  # MSE for regression\n",
        "#     train_mse_history.append(train_mse)\n",
        "#     train_r2 = r2_score(y_train_reg.numpy(), reg_pred.squeeze().detach().numpy())\n",
        "#     train_r2_history.append(train_r2)\n",
        "\n",
        "#     # Compute train accuracy\n",
        "#     predicted_labels_train = torch.argmax(class_pred, dim=1)\n",
        "#     train_accuracy = (predicted_labels_train == y_train_class).float().mean().item()\n",
        "#     train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "#     # Print results for the current epoch\n",
        "#     print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "#           f\"Train Classification Loss: {loss1.item():.4f}, Train Classification Accuracy: {train_accuracy:.4f}, \"\n",
        "#           f\"Train Regression Loss (MSE): {loss2.item():.4f}, Train Regression MSE: {train_mse:.4f}, \"\n",
        "#           f\"Train Regression R: {train_r2:.4f}\")\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         class_pred_test, reg_pred_test = model(X_test)\n",
        "\n",
        "#         # Compute test loss\n",
        "#         test_loss_class = loss_class(class_pred_test, y_test_class).item()\n",
        "#         test_loss_reg = loss_reg(reg_pred_test.squeeze(), y_test_reg).item()\n",
        "\n",
        "#         test_loss = test_loss_class + test_loss_reg\n",
        "#         test_loss_history.append(test_loss)\n",
        "\n",
        "#         # Compute test accuracy\n",
        "#         predicted_labels_test = torch.argmax(class_pred_test, dim=1)\n",
        "#         test_accuracy = (predicted_labels_test == y_test_class).float().mean().item()\n",
        "#         test_accuracy_history.append(test_accuracy)\n",
        "\n",
        "#         # Calculate R score for regression\n",
        "#         r2_test = r2_score(y_test_reg.numpy(), reg_pred_test.squeeze().numpy())\n",
        "#         test_r2_history.append(r2_test)\n",
        "\n",
        "#         # Calculate MSE for test data\n",
        "#         test_mse = loss_reg(reg_pred_test.squeeze(), y_test_reg).item()\n",
        "#         test_mse_history.append(test_mse)\n",
        "\n",
        "        \n",
        "#     # early_stopping(val_loss, model)    \n",
        "#     # if early_stopping.early_stop:\n",
        "#     #     print(\"Early stopping\")\n",
        "#     #     break\n",
        "#     # # After each epoch, print test results\n",
        "#     print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "#           f\"Test Classification Loss: {test_loss_class:.4f}, Test Classification Accuracy: {test_accuracy:.4f}, \"\n",
        "#           f\"Test Regression Loss (MSE): {test_loss_reg:.4f}, Test Regression R: {r2_test:.4f}, \"\n",
        "#           f\"Test MSE: {test_mse:.4f}\")\n",
        "\n",
        "#     # Switch back to train mode for the next epoch\n",
        "#     model.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0A35W-DFW9I",
        "outputId": "8d1a402c-8365-40a2-d25a-95c0f6ed510a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_loss_class' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Final results after all epochs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_test_loss_class \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loss_class\u001b[49m\n\u001b[0;32m      3\u001b[0m final_test_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy\n\u001b[0;32m      4\u001b[0m final_test_loss_reg \u001b[38;5;241m=\u001b[39m test_loss_reg\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_loss_class' is not defined"
          ]
        }
      ],
      "source": [
        "# Final results after all epochs\n",
        "final_test_loss_class = test_loss_class\n",
        "final_test_accuracy = test_accuracy\n",
        "final_test_loss_reg = test_loss_reg\n",
        "final_r2_score = test_r2\n",
        "\n",
        "# Final results print in desired format\n",
        "print(f\"Test Classification Loss: {final_test_loss_class:.4f}\")\n",
        "print(f\"Test Classification Accuracy: {final_test_accuracy:.4f}\")\n",
        "print(f\"Test Regression Loss (MSE): {final_test_loss_reg:.4f}\")\n",
        "print(f\"Test Regression R Score: {final_r2_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jAqMB5V3aG-r",
        "outputId": "8b2a9b49-66c5-4111-92a0-ebb33afeea90"
      },
      "outputs": [],
      "source": [
        "# --- Plot Training and Test Loss ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(train_loss_history, linestyle='-', label='Train Loss')\n",
        "plt.plot(test_loss_history, linestyle='-', label='Test Loss')\n",
        "plt.title('Training and Test Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Plot Training and Test MSE ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(train_mse_history, linestyle='-', label='Train MSE')\n",
        "plt.plot(test_mse_history, linestyle='-', label='Test MSE', color='orange')\n",
        "plt.title('Training and Test MSE Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Plot Train and Test Accuracy ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(train_accuracy_history, linestyle='-', label='Train Accuracy')\n",
        "plt.plot(test_accuracy_history, linestyle='-', label='Test Accuracy', color='orange')\n",
        "plt.title('Train and Test Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Plot Train and Test R Score ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(train_r2_history, linestyle='-', label='Train R Score')\n",
        "plt.plot(test_r2_history, linestyle='-', label='Test R Score', color='orange')\n",
        "plt.title('Train and Test R Score Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('R Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
