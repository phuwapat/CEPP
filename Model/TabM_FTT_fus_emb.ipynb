{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fff45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random, torch.backends.cudnn as cudnn\n",
    "import re\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rtdl_revisiting_models import  FTTransformer\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader, Dataset\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from tabm import TabM, EnsembleView, make_tabm_backbone, LinearEnsemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b172c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"convert_json_data.csv\")\n",
    "# df = df.iloc[:50_000].copy()\n",
    "LABEL_ONLY_COLS = [\"category_group\"]\n",
    "CATEGORICAL_FEATURES = [        \n",
    "    'currency',\n",
    "    'country_displayable_name',\n",
    "    'location_state',\n",
    "]\n",
    "NUMERIC_FEATURES = [\n",
    "    'goal_usd_log',\n",
    "    'name_len',\n",
    "    'blurb_len',\n",
    "    'has_video', \n",
    "    'days_diff_launched_at_deadline_log',\n",
    "    'goal_per_day_log',\n",
    "    'goal_rank_in_cat',\n",
    "    'goal_vs_cat_median',\n",
    "    'goal_vs_country_median',\n",
    "    'goal_round_100',\n",
    "    'goal_round_1000',\n",
    "    'cat_freq',\n",
    "    'country_freq',\n",
    "    'cat_x_country_freq',\n",
    "    'gpd_rank_in_cat',\n",
    "    'gpd_vs_cat_median',\n",
    "    'gpd_dist_cat_median',\n",
    "    'cat_country_share',\n",
    "    'prep_days',\n",
    "    'has_photo',\n",
    "    'launch_dow',\n",
    "    'deadline_dow',\n",
    "    'too_short_or_long',\n",
    "]\n",
    "CYCLIC_NUMERIC = [ \n",
    "    'deadline_mon_sin',\n",
    "    'deadline_mon_cos',\n",
    "    'deadline_dom_sin',\n",
    "    'deadline_dom_cos',  \n",
    "    'launched_at_mon_sin',\n",
    "    'launched_at_mon_cos',\n",
    "    'launched_at_dom_sin',\n",
    "    'launched_at_dom_cos',  \n",
    "]\n",
    "\n",
    "FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb93af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    df[col] = df[col].fillna(\"missing\").astype(str)\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "for col in LABEL_ONLY_COLS:\n",
    "    df[col] = df[col].fillna(\"missing\").astype(str)\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col]) \n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda84386",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE = {\n",
    "    \"success_cls\": df[\"state\"].to_numpy(dtype=np.int64),\n",
    "    # \"success_rate_cls\": df[\"success_rate_cls\"].to_numpy(dtype=np.int64),\n",
    "    \"risk_level\": df[\"risk_level\"].to_numpy(dtype=np.int64),\n",
    "    \"days_to_state_change\": df[\"duration_class\"].to_numpy(dtype=np.int64),\n",
    "    \"recommend_category\": df[\"category_group\"].to_numpy(dtype=np.int64),\n",
    "    \"goal_eval\": df[\"goal_eval\"].to_numpy(dtype=np.int64),\n",
    "    # \"shortfall_severity_cls\": df[\"shortfall_severity_cls\"].to_numpy(dtype=np.int64),\n",
    "    \"stretch_potential_cls\": df[\"stretch_potential_cls\"].to_numpy(dtype=np.int64),\n",
    "}\n",
    "\n",
    "key = df[\"category_group\"].astype(str) + \"_\" + df[\"state\"].astype(str)\n",
    "idx_train, idx_val = train_test_split(np.arange(len(df)), test_size=0.2, random_state=42, stratify=key)\n",
    "\n",
    "X_train_df = df.iloc[idx_train]\n",
    "X_val_df = df.iloc[idx_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2a0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283698be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FINAL cat_cardinalities = [15, 25, 710]\n"
     ]
    }
   ],
   "source": [
    "cat_cardinalities = [int(X_train_df[col].max()) + 1 for col in CATEGORICAL_FEATURES]\n",
    "print(\"✅ FINAL cat_cardinalities =\", cat_cardinalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b634a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_cat, x_cont, y_dict):\n",
    "        self.x_cat = x_cat\n",
    "        self.x_cont = x_cont\n",
    "        self.y_dict = y_dict\n",
    "        self.keys = list(y_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_cat)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.x_cat[idx],\n",
    "            self.x_cont[idx],\n",
    "            {k: self.y_dict[k][idx] for k in self.keys}\n",
    "        )\n",
    "\n",
    "y_train_dict = {\n",
    "    k: torch.tensor(v[idx_train]) for k, v in TARGET_FEATURE.items()\n",
    "}\n",
    "y_val_dict = {\n",
    "    k: torch.tensor(v[idx_val]) for k, v in TARGET_FEATURE.items()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_train = (\n",
    "    X_train_df[\"name\"].fillna(\"\").str.lower() + \" \" +\n",
    "    X_train_df[\"blurb\"].fillna(\"\").str.lower() + \" \" +\n",
    "    X_train_df[\"category_slug\"].fillna(\"\").str.replace(\"/\", \" \").str.lower()\n",
    ").tolist()\n",
    "\n",
    "combined_val = (\n",
    "    X_val_df[\"name\"].fillna(\"\").str.lower() + \" \" +\n",
    "    X_val_df[\"blurb\"].fillna(\"\").str.lower() + \" \" +\n",
    "    X_val_df[\"category_slug\"].fillna(\"\").str.replace(\"/\", \" \").str.lower()\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.9, ngram_range=(1,2), stop_words='english')\n",
    "X_tfidf_tr = tfidf.fit_transform(combined_train)\n",
    "X_tfidf_va = tfidf.transform(combined_val)\n",
    "\n",
    "svd_dim = 128\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "X_txt_tr = svd.fit_transform(X_tfidf_tr)\n",
    "X_txt_va = svd.transform(X_tfidf_va)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(X_train_df[NUMERIC_FEATURES])\n",
    "numeric_scaled_val = scaler.transform(X_val_df[NUMERIC_FEATURES])\n",
    "\n",
    "X_cont_train = np.concatenate([numeric_scaled, X_txt_tr, X_train_df[CYCLIC_NUMERIC]], axis=1)\n",
    "X_cont_val = np.concatenate([numeric_scaled_val, X_txt_va, X_val_df[CYCLIC_NUMERIC]], axis=1)\n",
    "\n",
    "x_cont_tensor = torch.tensor(X_cont_train, dtype=torch.float32)\n",
    "x_cont_val = torch.tensor(X_cont_val, dtype=torch.float32)\n",
    "\n",
    "x_cat_tensor = torch.tensor(X_train_df[CATEGORICAL_FEATURES].values, dtype=torch.long)\n",
    "x_cat_val = torch.tensor(X_val_df[CATEGORICAL_FEATURES].values, dtype=torch.long)\n",
    "\n",
    "train_ds = MultiTaskDataset(x_cat_tensor, x_cont_tensor, y_train_dict)\n",
    "val_ds = MultiTaskDataset(x_cat_val, x_cont_val, y_val_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== DataLoaders =====\n",
    "BATCH_SIZE = 256  # ของคุณเดิม\n",
    "idx = np.arange(len(train_ds))\n",
    "# ปกติ (ไม่ oversample) ใช้เฉพาะช่วง warmup และเอาไว้เทียบ\n",
    "train_loader_plain = DataLoader(train_ds,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                drop_last=False)\n",
    "\n",
    "# Validation ไม่แตะ (ห้าม oversample)\n",
    "val_loader = DataLoader(val_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0316af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success_cls': 2, 'risk_level': 3, 'days_to_state_change': 4, 'recommend_category': 10, 'goal_eval': 3, 'stretch_potential_cls': 3}\n"
     ]
    }
   ],
   "source": [
    "for k in y_train_dict:\n",
    "    y_train_dict[k] = y_train_dict[k].long()\n",
    "for k in y_val_dict:\n",
    "    y_val_dict[k] = y_val_dict[k].long()\n",
    "\n",
    "num_classes_map = {\n",
    "    k: int(y_train_dict[k].max().item() + 1)\n",
    "    for k in y_train_dict.keys()\n",
    "}\n",
    "print(num_classes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2974bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadWrapper(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, head_dims: dict, in_dim: int, d_hidden: int = 64):\n",
    "        super().__init__()\n",
    "        self.backbone = base_model\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "        def make_head(out_dim, hidden=d_hidden, dropout=0.2):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, out_dim),\n",
    "            )\n",
    "\n",
    "        # ---- หัวหลัก (ประกาศแบบแยก ไม่ใช้ลูป) ----\n",
    "        self.head_success_cls = make_head(head_dims[\"success_cls\"]) if \"success_cls\" in head_dims else None\n",
    "        self.head_risk_level = make_head(head_dims[\"risk_level\"]) if \"risk_level\" in head_dims else None\n",
    "\n",
    "        self.head_days_to_state_change = (\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, head_dims[\"days_to_state_change\"]),\n",
    "            ) if \"days_to_state_change\" in head_dims else None\n",
    "        )\n",
    "\n",
    "        self.head_recommend_category = (\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, head_dims[\"recommend_category\"]),\n",
    "            ) if \"recommend_category\" in head_dims else None\n",
    "        )\n",
    "\n",
    "        self.head_goal_eval = make_head(head_dims[\"goal_eval\"]) if \"goal_eval\" in head_dims else None\n",
    "\n",
    "        # ---- 3 หัวใหม่ (มีคีย์ถึงจะสร้าง) ----\n",
    "        self.head_stretch_potential_cls = (\n",
    "            make_head(head_dims[\"stretch_potential_cls\"]) if \"stretch_potential_cls\" in head_dims else None\n",
    "        )\n",
    "        self.head_shortfall_severity_cls = (\n",
    "            make_head(head_dims[\"shortfall_severity_cls\"]) if \"shortfall_severity_cls\" in head_dims else None\n",
    "        )\n",
    "        self.head_near_miss_cls = (\n",
    "            make_head(head_dims[\"near_miss_cls\"]) if \"near_miss_cls\" in head_dims else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cont, x_cat):\n",
    "        x = self.backbone(x_cont, x_cat)  # [B, in_dim]\n",
    "        out = {}\n",
    "\n",
    "        if self.head_success_cls is not None:\n",
    "            out[\"success_cls\"] = self.head_success_cls(x)\n",
    "        if self.head_risk_level is not None:\n",
    "            out[\"risk_level\"] = self.head_risk_level(x)\n",
    "        if self.head_days_to_state_change is not None:\n",
    "            out[\"days_to_state_change\"] = self.head_days_to_state_change(x)\n",
    "        if self.head_recommend_category is not None:\n",
    "            out[\"recommend_category\"] = self.head_recommend_category(x)\n",
    "        if self.head_goal_eval is not None:\n",
    "            out[\"goal_eval\"] = self.head_goal_eval(x)\n",
    "\n",
    "        if self.head_stretch_potential_cls is not None:\n",
    "            out[\"stretch_potential_cls\"] = self.head_stretch_potential_cls(x)\n",
    "        if self.head_shortfall_severity_cls is not None:\n",
    "            out[\"shortfall_severity_cls\"] = self.head_shortfall_severity_cls(x)\n",
    "        if self.head_near_miss_cls is not None:\n",
    "            out[\"near_miss_cls\"] = self.head_near_miss_cls(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563eb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tabm import EnsembleView, make_tabm_backbone, LinearEnsemble\n",
    "class _MLP(nn.Module):\n",
    "    def __init__(self, d_in, out_dim, hidden=128, depth=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers, d = [], d_in\n",
    "        for _ in range(depth-1):\n",
    "            layers += [nn.Linear(d, hidden), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = hidden\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):  # x: (B, d_in)\n",
    "        return self.net(x)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, d_in, out_dim, k, hidden=128, depth=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mlps = nn.ModuleList([_MLP(d_in, out_dim, hidden, depth, dropout) for _ in range(k)])\n",
    "    def forward(self, z):  # z: (B, k, d_in)\n",
    "        outs = [self.mlps[i](z[:, i, :]) for i in range(len(self.mlps))]\n",
    "        return torch.stack(outs, dim=1)  # (B, k, C)\n",
    "    \n",
    "class MLPHeadShared(nn.Module):\n",
    "    def __init__(self, d_in, out_dim, hidden=128, depth=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = d_in\n",
    "        for _ in range(depth-1):\n",
    "            layers += [nn.Linear(d, hidden), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = hidden\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):            # z: (B, k, d_in)\n",
    "        B, k, D = z.shape\n",
    "        y = self.net(z.reshape(B*k, D))\n",
    "        return y.view(B, k, -1)  \n",
    "# ---- Multi-head สำหรับ TabM (ใช้ LinearEnsemble แบบตำแหน่ง) ----\n",
    "class TabMHeads(nn.Module):\n",
    "    def __init__(self, d_in: int, k: int, head_dims: dict[str, int]):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.heads = nn.ModuleDict({\n",
    "            name: LinearEnsemble(d_in, out_dim, k=k)  # <- ไม่มี d_in/d_out เป็นคีย์เวิร์ด\n",
    "            for name, out_dim in head_dims.items()\n",
    "        })\n",
    "        # self.heads = nn.ModuleDict({\n",
    "        #     name: MLPHeadShared(d_in, out_dim, hidden=128, depth=2, dropout=0.1)\n",
    "        #     for name, out_dim in head_dims.items()\n",
    "        # })\n",
    "        # self.heads = nn.ModuleDict({\n",
    "        #     name: MLPEnsemble(d_in, out_dim, k=k, hidden=128, depth=2, dropout=0.1)\n",
    "        #     for name, out_dim in head_dims.items()\n",
    "        # })\n",
    "\n",
    "    def forward(self, z):  # z: (B, k, d_in)\n",
    "        return {name: head(z) for name, head in self.heads.items()}  # (B, k, C) ต่อหัว\n",
    "\n",
    "# ---- Backbone + multi-head ของ TabM ----\n",
    "class TabMBackboneMultiHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_num_features: int,\n",
    "        cat_cardinalities: list[int] | None,\n",
    "        head_dims: dict[str, int],\n",
    "        k: int = 8,\n",
    "        d_block: int = 256,\n",
    "        n_blocks: int = 4,\n",
    "        dropout: float = 0.0,\n",
    "        start_scaling_init: str = \"normal\",\n",
    "        start_scaling_init_chunks=None,  # ใส่ None ถ้าอินพุตเป็นก้อนเดียว\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.n_num = n_num_features\n",
    "        self.cats = list(cat_cardinalities or [])\n",
    "        d_in = n_num_features + sum(self.cats)  # one-hot cat ภายในโมเดล\n",
    "\n",
    "        self.ensemble_view = EnsembleView(k=k)\n",
    "        self.backbone = make_tabm_backbone(\n",
    "            d_in=d_in,\n",
    "            d_block=d_block,\n",
    "            n_blocks=n_blocks,\n",
    "            dropout=dropout,\n",
    "            k=k,\n",
    "            start_scaling_init=start_scaling_init,\n",
    "            start_scaling_init_chunks=start_scaling_init_chunks,\n",
    "        )\n",
    "        self.heads = TabMHeads(d_in=d_block, k=k, head_dims=head_dims)\n",
    "\n",
    "    def _one_hot_cat(self, x_cat: torch.Tensor | None):\n",
    "        if x_cat is None or len(self.cats) == 0:\n",
    "            return None\n",
    "        oh = [F.one_hot(x_cat[:, i].long(), num_classes=c) for i, c in enumerate(self.cats)]\n",
    "        return torch.cat(oh, dim=-1).float()\n",
    "\n",
    "    def forward(self, x_num: torch.Tensor, x_cat: torch.Tensor | None = None):\n",
    "        # x_num: (B, n_num), x_cat: (B, n_cat) เป็นดัชนี 0..card-1\n",
    "        if x_cat is not None and len(self.cats):\n",
    "            x = torch.cat([x_num, self._one_hot_cat(x_cat)], dim=-1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        x = self.ensemble_view(x)     # (B, k, D)\n",
    "        z = self.backbone(x)          # (B, k, d_block)\n",
    "        return self.heads(z)          # dict: name -> (B, k, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a24347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgFusionBackbone(nn.Module):\n",
    "    def __init__(self, tabm, ftt, d_tabm, d_ftt, d_out, learn_alpha=False, alpha_init=0.5):\n",
    "        super().__init__()\n",
    "        self.tabm = tabm\n",
    "        self.ftt  = ftt\n",
    "        self.d_out  = d_out \n",
    "        self.proj_tabm = nn.Linear(d_tabm, d_out, bias=False)\n",
    "        self.proj_ftt  = nn.Linear(d_ftt,  d_out, bias=False)\n",
    "        self.norm1 = nn.LayerNorm(d_out)\n",
    "        self.norm2 = nn.LayerNorm(d_out)\n",
    "\n",
    "        # ถ้าอยากเรียนรู้ alpha ให้เป็นพารามิเตอร์ 0..1 (ผ่าน sigmoid)\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha_init)) if learn_alpha else None\n",
    "\n",
    "        self._last_z1 = None\n",
    "        self._last_z2 = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _first_tensor(obj):\n",
    "        import torch\n",
    "        if torch.is_tensor(obj):\n",
    "            return obj\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            for v in obj:\n",
    "                if torch.is_tensor(v):\n",
    "                    return v\n",
    "        if isinstance(obj, dict):\n",
    "            # ลำดับคีย์ที่มักใช้เก็บ embedding/feature\n",
    "            for k in (\"emb\", \"features\", \"feature\", \"hidden\", \"h\", \"x\", \"repr\", \"tokens\", \"out\", \"logits\"):\n",
    "                v = obj.get(k, None)\n",
    "                if torch.is_tensor(v):\n",
    "                    return v\n",
    "            # เผื่อค่าด้านใน dict เป็นเทนเซอร์ตรง ๆ\n",
    "            for v in obj.values():\n",
    "                if torch.is_tensor(v):\n",
    "                    return v\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _call_any_signature(model, x_cont, x_cat):\n",
    "        # พยายามหลาย signature\n",
    "        try:\n",
    "            return model(x_cont, x_cat)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return model(x_cont)\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    return model(x_num=x_cont, x_cat=x_cat)\n",
    "                except TypeError:\n",
    "                    return model(x_cont)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_emb(model, x_cont, x_cat, name=\"model\"):\n",
    "        # -------- เคส TabM (มี ensemble_view + backbone) --------\n",
    "        if hasattr(model, \"ensemble_view\") and hasattr(model, \"backbone\"):\n",
    "            dev = next(model.backbone.parameters()).device\n",
    "            x_cont = x_cont.to(dev)\n",
    "            if x_cat is not None:\n",
    "                x_cat = x_cat.to(dev)\n",
    "            if x_cat is not None and getattr(model, \"cats\", None):\n",
    "                x = torch.cat([x_cont, model._one_hot_cat(x_cat)], dim=-1)\n",
    "            else:\n",
    "                x = x_cont\n",
    "            x = model.ensemble_view(x)    # (B, k, D)\n",
    "            z = model.backbone(x)         # (B, k, d_block)\n",
    "            return z.mean(dim=1)          # (B, d_block)\n",
    "\n",
    "        # -------- โมเดลทั่วไป (เช่น FTT) --------\n",
    "        dev = next(model.parameters()).device\n",
    "        x_cont = x_cont.to(dev)\n",
    "        if x_cat is not None:\n",
    "            x_cat = x_cat.to(dev)\n",
    "\n",
    "        # 1) ลอง forward ปกติหลายแบบ\n",
    "        out = AvgFusionBackbone._call_any_signature(model, x_cont, x_cat)\n",
    "        t = AvgFusionBackbone._first_tensor(out)\n",
    "\n",
    "        # 2) ถ้ายังไม่ได้ ให้ลองเมธอดที่พบบ่อย\n",
    "        if t is None:\n",
    "            for attr in (\"forward_features\", \"encode\", \"extract\", \"backbone\", \"encoder\"):\n",
    "                if hasattr(model, attr):\n",
    "                    fn = getattr(model, attr)\n",
    "                    try:\n",
    "                        out2 = fn(x_cont, x_cat)\n",
    "                    except TypeError:\n",
    "                        try:\n",
    "                            out2 = fn(x_cont)\n",
    "                        except TypeError:\n",
    "                            continue\n",
    "                    t = AvgFusionBackbone._first_tensor(out2)\n",
    "                    if t is not None:\n",
    "                        break\n",
    "\n",
    "        # 3) ถ้ายังไม่ได้อีก ให้แจ้ง error ชัด ๆ\n",
    "        if t is None:\n",
    "            raise TypeError(\n",
    "                f\"{name}.forward/encode did not return a Tensor. \"\n",
    "                f\"Please expose a feature method (e.g., forward_features) or return logits/features.\"\n",
    "            )\n",
    "\n",
    "        # 4) ถ้าเป็น (B, k, D) ให้เฉลี่ย k\n",
    "        if t.dim() == 3:\n",
    "            t = t.mean(dim=1)\n",
    "        return t\n",
    "        \n",
    "    def forward(self, x_cont, x_cat):\n",
    "        z1 = self._get_emb(self.tabm, x_cont, x_cat, name=\"tabm\")  # (B, d_tabm)\n",
    "        z2 = self._get_emb(self.ftt,  x_cont, x_cat, name=\"ftt\")   # (B, d_ftt)\n",
    "\n",
    "        z1 = F.normalize(self.norm1(self.proj_tabm(z1)), dim=-1)\n",
    "        z2 = F.normalize(self.norm2(self.proj_ftt(z2)), dim=-1)\n",
    "\n",
    "        if self.alpha is None:\n",
    "            z = 0.5 * (z1 + z2)\n",
    "        else:\n",
    "            a = torch.clamp(self.alpha, 0, 1)\n",
    "            z = a * z1 + (1 - a) * z2\n",
    "\n",
    "        self._last_z1, self._last_z2 = z1, z2\n",
    "        return z  # (B, d_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78618f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tabm.py:1481: UserWarning: start_scaling_init_chunks is not provided, which may lead to suboptimal performance in some cases\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_model_out_dim = 512\n",
    "n_cont_features = X_cont_train.shape[1] \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "head_dims = {\n",
    "    \"success_cls\": 2,\n",
    "    # \"success_rate_cls\": 6,\n",
    "    \"risk_level\": 3,\n",
    "    \"days_to_state_change\": 4,\n",
    "    \"recommend_category\": len(label_encoders[\"category_group\"].classes_),\n",
    "    \"goal_eval\": 3,\n",
    "    # \"shortfall_severity_cls\": 4,\n",
    "    \"stretch_potential_cls\": 3,\n",
    "}\n",
    "tabm_enc = TabMBackboneMultiHead(\n",
    "    n_num_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,    # ถ้าใช้ one-hot ไปแล้ว ให้ใส่ None และไม่ส่ง x_cat ตอน forward\n",
    "    head_dims=head_dims,\n",
    "    k=14,\n",
    "    d_block=d_model_out_dim,\n",
    "    n_blocks=5,\n",
    "    dropout=0.3,\n",
    "    start_scaling_init=\"normal\",            # ค่านิยมใน ref\n",
    "    start_scaling_init_chunks=None,         # หรือ [n_cont_features, sum(emb_dim)] เพื่อสเกลแยกกลุ่ม\n",
    ").to(device)\n",
    "ftt_enc = FTTransformer(\n",
    "    n_cont_features=n_cont_features,  # ใช้ตามจริงจาก data\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    d_out=128,\n",
    "    n_blocks=5,\n",
    "    d_block=d_model_out_dim,\n",
    "    attention_n_heads=2,\n",
    "    attention_dropout=0.25,\n",
    "    ffn_d_hidden_multiplier=4 / 3,\n",
    "    ffn_dropout=0.25,\n",
    "    residual_dropout=0.1,\n",
    ").to(device)\n",
    "fuse_backbone = AvgFusionBackbone(tabm_enc, ftt_enc,\n",
    "                                  d_tabm=512,\n",
    "                                  d_ftt=128,\n",
    "                                  d_out=256)\n",
    "\n",
    "model = MultiHeadWrapper(\n",
    "    base_model=fuse_backbone,   # <<<<<< ใช้ตัวรวมเป็น backbone\n",
    "    head_dims=head_dims,\n",
    "    in_dim=fuse_backbone.d_out,\n",
    "    d_hidden=64\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5153f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on: cuda:0\n",
      "CUDA available: True\n",
      "2.7.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Model is running on:\", next(model.parameters()).device)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e22609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dropout(x_cont, drop_prob=0.1):\n",
    "    if drop_prob <= 0: return x_cont\n",
    "    mask = (torch.rand_like(x_cont) < drop_prob).float()\n",
    "    return x_cont * (1 - mask)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46e2cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDINAL_HEADS = {}\n",
    "def corn_targets(y, num_classes):\n",
    "    B = y.size(0); K = num_classes\n",
    "    t = torch.arange(K-1, device=y.device).unsqueeze(0).expand(B, -1)\n",
    "    return (y.unsqueeze(1) > t).float()\n",
    "\n",
    "def corn_loss(logits, y, pos_weight=None):\n",
    "    tgt = corn_targets(y, logits.size(1) + 1)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        logits, tgt, pos_weight=pos_weight, reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "def corn_predict(logits):\n",
    "    p = torch.sigmoid(logits)\n",
    "    return (p > 0.5).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fde69b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SORD_HEADS = {}   \n",
    "def _sord_targets(y, C, tau=0.75, device=None):\n",
    "    y = y.view(-1, 1).long()\n",
    "    cls = torch.arange(C, device=device).view(1, -1)\n",
    "    dist = (cls - y).abs().float()\n",
    "    p = torch.softmax(-dist / tau, dim=1)\n",
    "    return p\n",
    "\n",
    "_kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "def sord_loss(logits, y, tau=0.75):\n",
    "    C = logits.size(1)\n",
    "    p = _sord_targets(y, C, tau=tau, device=logits.device)   # soft target\n",
    "    return _kl(F.log_softmax(logits, dim=1), p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "174362de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001C4A52DB610>\n"
     ]
    }
   ],
   "source": [
    "class AutomaticWeightedLoss(nn.Module):\n",
    "    \"\"\"automatically weighted multi-task loss\n",
    "\n",
    "    Params：\n",
    "        num: int，the number of loss\n",
    "        x: multi-task loss\n",
    "    Examples：\n",
    "        loss1=1\n",
    "        loss2=2\n",
    "        awl = AutomaticWeightedLoss(2)\n",
    "        loss_sum = awl(loss1, loss2)\n",
    "    \"\"\"\n",
    "    def __init__(self, num=2):\n",
    "        super(AutomaticWeightedLoss, self).__init__()\n",
    "        params = torch.ones(num, requires_grad=True)\n",
    "        self.params = torch.nn.Parameter(params)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        loss_sum = 0\n",
    "        for i, loss in enumerate(x):\n",
    "            loss_sum += 0.5 / (self.params[i] ** 2) * loss + torch.log(1 + self.params[i] ** 2)\n",
    "        return loss_sum\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    awl = AutomaticWeightedLoss(2)\n",
    "    print(awl.parameters())\n",
    "    \n",
    "PRIOR_HEADS  = {\"stretch_potential_cls\", \"risk_level\"}\n",
    "HEADS_ORDER  = [\"success_cls\", \"risk_level\", \"days_to_state_change\",\n",
    "                \"recommend_category\", \"goal_eval\", \"stretch_potential_cls\"]\n",
    "OTHERS_ORDER = [h for h in HEADS_ORDER if h not in PRIOR_HEADS]\n",
    "\n",
    "# สร้าง AWL ด้วยจำนวนหัวที่ตรงกับ OTHERS_ORDER\n",
    "awl = AutomaticWeightedLoss(len(OTHERS_ORDER)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41152808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight: torch.Tensor | None = None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = float(gamma)\n",
    "        self.reduction = reduction\n",
    "        # เก็บ weight เป็น buffer (ย้าย device ตามโมดูล, ไม่อัปเดตกราด)\n",
    "        if weight is not None:\n",
    "            self.register_buffer('weight', weight.float())\n",
    "        else:\n",
    "            self.weight = None\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor):\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        p = logp.exp()\n",
    "        w = self.weight\n",
    "        if w is not None and w.device != logits.device:\n",
    "            w = w.to(logits.device)\n",
    "        loss = F.nll_loss(((1 - p) ** self.gamma) * logp,\n",
    "                          target, weight=w, reduction='none')\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "# --- คำนวณ class weight จาก label ทั้งชุด (ไม่ต้องอ้าง logits) ---\n",
    "def class_weights_from_labels(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    y_cpu = y.detach().view(-1).cpu().long()\n",
    "    cnt = torch.bincount(y_cpu, minlength=num_classes).float()\n",
    "    inv = cnt.sum() / cnt.clamp_min(1)     # inverse frequency\n",
    "    w = inv / inv.mean()                    # normalize ให้มีค่าเฉลี่ย ~1\n",
    "    return w\n",
    "\n",
    "# ตัวอย่างการสร้าง weight และผูกเข้ากับ loss ฟังก์ชัน\n",
    "C_risk    = num_classes_map['risk_level']\n",
    "C_stretch = num_classes_map['stretch_potential_cls']\n",
    "\n",
    "w_risk    = class_weights_from_labels(y_train_dict['risk_level'],            C_risk)\n",
    "w_stretch = class_weights_from_labels(y_train_dict['stretch_potential_cls'], C_stretch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cfaa063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _repeat_k_targets(target: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    # target: (B,) -> (B*k,)\n",
    "    return target.view(-1, 1).repeat(1, k).reshape(-1)\n",
    "\n",
    "def _tabm_ce_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # pred: (B, k, C)\n",
    "    B, k, C = pred.shape\n",
    "    return F.cross_entropy(pred.reshape(B*k, C), _repeat_k_targets(target, k).long())\n",
    "\n",
    "def _tabm_sord_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # pred: (B, k, C) , ฟังก์ชัน sord_loss ของคุณคาด (B, C)\n",
    "    B, k, C = pred.shape\n",
    "    return sord_loss(pred.reshape(B*k, C), _repeat_k_targets(target, k))\n",
    "\n",
    "def _tabm_corn_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # pred: (B, k, C-1) สำหรับ CORN\n",
    "    B, k, D = pred.shape\n",
    "    return corn_loss(pred.reshape(B*k, D), _repeat_k_targets(target, k))\n",
    "\n",
    "def _ensemble_probs(pred_logits: torch.Tensor) -> torch.Tensor:\n",
    "    # ถ้าเป็น (B,k,C) -> เฉลี่ย probs ข้าม k, ถ้า (B,C) -> softmax ตรงๆ\n",
    "    if pred_logits.dim() == 3:\n",
    "        return pred_logits.softmax(-1).mean(1)  # (B, C)\n",
    "    return pred_logits.softmax(-1)              # (B, C)\n",
    "\n",
    "def _ensemble_argmax(pred_logits: torch.Tensor) -> torch.Tensor:\n",
    "    return _ensemble_probs(pred_logits).argmax(-1)\n",
    "\n",
    "def _ensemble_corn_predict(pred_logits: torch.Tensor) -> torch.Tensor:\n",
    "    # ทางง่ายและได้ผลดี: เอา logits เฉลี่ยข้าม k แล้วใช้ corn_predict เดิม\n",
    "    if pred_logits.dim() == 3:\n",
    "        return corn_predict(pred_logits.mean(1))\n",
    "    return corn_predict(pred_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9bbec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 00] AWL inverse-var weights: {'success_cls': 1.0, 'days_to_state_change': 1.0, 'recommend_category': 1.0, 'goal_eval': 1.0, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 00 | Train Loss: 6.8440 | Val Loss: 6.0810\n",
      "  - success_cls          loss: 0.6365 | val_loss: 0.6029 raw_loss: 0.6365 | raw_val: 0.6029 | train_acc: 0.6488 | val_acc: 0.6858 | train_F1: 0.5159 | val_F1: 0.6376 | train_P: 0.6451 | val_P: 0.6745 | train_R: 0.5718 | val_R: 0.6370\n",
      "  - risk_level           loss: 1.0706 | val_loss: 1.0444 raw_loss: 1.0706 | raw_val: 1.0444 | train_acc: 0.4140 | val_acc: 0.4546 | train_F1: 0.3583 | val_F1: 0.4545 | train_P: 0.4095 | val_P: 0.4631 | train_R: 0.4140 | val_R: 0.4544\n",
      "  - days_to_state_change loss: 1.0785 | val_loss: 0.7711 raw_loss: 1.0785 | raw_val: 0.7711 | train_acc: 0.4920 | val_acc: 0.6197 | train_F1: 0.4360 | val_F1: 0.5753 | train_P: 0.4695 | val_P: 0.6145 | train_R: 0.4921 | val_R: 0.6206\n",
      "  - recommend_category   loss: 2.2856 | val_loss: 2.2392 raw_loss: 2.2856 | raw_val: 2.2392 | train_acc: 0.1200 | val_acc: 0.1677 | train_F1: 0.0698 | val_F1: 0.0977 | train_P: 0.1085 | val_P: 0.0865 | train_R: 0.1200 | val_R: 0.1676\n",
      "  - goal_eval            loss: 0.7245 | val_loss: 0.4190 raw_loss: 0.7245 | raw_val: 0.4190 | train_acc: 0.6797 | val_acc: 0.8668 | train_F1: 0.5227 | val_F1: 0.8418 | train_P: 0.6658 | val_P: 0.8517 | train_R: 0.5611 | val_R: 0.8386\n",
      "  - stretch_potential_cls loss: 1.0481 | val_loss: 1.0045 raw_loss: 1.0481 | raw_val: 1.0045 | train_acc: 0.4528 | val_acc: 0.5098 | train_F1: 0.3400 | val_F1: 0.4890 | train_P: 0.4423 | val_P: 0.4931 | train_R: 0.4181 | val_R: 0.4960\n",
      "[epoch 01] AWL inverse-var weights: {'success_cls': 1.0, 'days_to_state_change': 1.0, 'recommend_category': 1.0, 'goal_eval': 1.0, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 01 | Train Loss: 5.6976 | Val Loss: 5.4637\n",
      "  - success_cls          loss: 0.5713 | val_loss: 0.5408 raw_loss: 0.5713 | raw_val: 0.5408 | train_acc: 0.7171 | val_acc: 0.7363 | train_F1: 0.6864 | val_F1: 0.6977 | train_P: 0.7048 | val_P: 0.7400 | train_R: 0.6835 | val_R: 0.6917\n",
      "  - risk_level           loss: 1.0213 | val_loss: 1.0091 raw_loss: 1.0213 | raw_val: 1.0091 | train_acc: 0.4762 | val_acc: 0.4814 | train_F1: 0.4718 | val_F1: 0.4792 | train_P: 0.4792 | val_P: 0.4988 | train_R: 0.4762 | val_R: 0.4811\n",
      "  - days_to_state_change loss: 0.5623 | val_loss: 0.5928 raw_loss: 0.5623 | raw_val: 0.5928 | train_acc: 0.7454 | val_acc: 0.7619 | train_F1: 0.7334 | val_F1: 0.7645 | train_P: 0.7404 | val_P: 0.7883 | train_R: 0.7461 | val_R: 0.7612\n",
      "  - recommend_category   loss: 2.1941 | val_loss: 2.1006 raw_loss: 2.1941 | raw_val: 2.1006 | train_acc: 0.1888 | val_acc: 0.2249 | train_F1: 0.1300 | val_F1: 0.1577 | train_P: 0.1549 | val_P: 0.1737 | train_R: 0.1888 | val_R: 0.2248\n",
      "  - goal_eval            loss: 0.3813 | val_loss: 0.2900 raw_loss: 0.3813 | raw_val: 0.2900 | train_acc: 0.8581 | val_acc: 0.9107 | train_F1: 0.8303 | val_F1: 0.8977 | train_P: 0.8364 | val_P: 0.8929 | train_R: 0.8312 | val_R: 0.9046\n",
      "  - stretch_potential_cls loss: 0.9673 | val_loss: 0.9305 raw_loss: 0.9673 | raw_val: 0.9305 | train_acc: 0.5294 | val_acc: 0.5544 | train_F1: 0.5108 | val_F1: 0.5475 | train_P: 0.5171 | val_P: 0.5551 | train_R: 0.5166 | val_R: 0.5475\n",
      "[epoch 02] AWL inverse-var weights: {'success_cls': 1.0, 'days_to_state_change': 1.0, 'recommend_category': 1.0, 'goal_eval': 1.0, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 02 | Train Loss: 5.2887 | Val Loss: 4.9080\n",
      "  - success_cls          loss: 0.5284 | val_loss: 0.5192 raw_loss: 0.5284 | raw_val: 0.5192 | train_acc: 0.7454 | val_acc: 0.7507 | train_F1: 0.7235 | val_F1: 0.7326 | train_P: 0.7346 | val_P: 0.7387 | train_R: 0.7198 | val_R: 0.7303\n",
      "  - risk_level           loss: 0.9988 | val_loss: 0.9906 raw_loss: 0.9988 | raw_val: 0.9906 | train_acc: 0.4901 | val_acc: 0.4895 | train_F1: 0.4849 | val_F1: 0.4785 | train_P: 0.4926 | val_P: 0.4939 | train_R: 0.4901 | val_R: 0.4890\n",
      "  - days_to_state_change loss: 0.4789 | val_loss: 0.3018 raw_loss: 0.4789 | raw_val: 0.3018 | train_acc: 0.8077 | val_acc: 0.8901 | train_F1: 0.8075 | val_F1: 0.8872 | train_P: 0.8098 | val_P: 0.8911 | train_R: 0.8078 | val_R: 0.8888\n",
      "  - recommend_category   loss: 2.0736 | val_loss: 1.9709 raw_loss: 2.0736 | raw_val: 1.9709 | train_acc: 0.2356 | val_acc: 0.2673 | train_F1: 0.1816 | val_F1: 0.2098 | train_P: 0.1931 | val_P: 0.2126 | train_R: 0.2353 | val_R: 0.2671\n",
      "  - goal_eval            loss: 0.2914 | val_loss: 0.2269 raw_loss: 0.2914 | raw_val: 0.2269 | train_acc: 0.8913 | val_acc: 0.9167 | train_F1: 0.8704 | val_F1: 0.9030 | train_P: 0.8740 | val_P: 0.9183 | train_R: 0.8699 | val_R: 0.8924\n",
      "  - stretch_potential_cls loss: 0.9175 | val_loss: 0.8986 raw_loss: 0.9175 | raw_val: 0.8986 | train_acc: 0.5593 | val_acc: 0.5793 | train_F1: 0.5404 | val_F1: 0.5575 | train_P: 0.5476 | val_P: 0.5679 | train_R: 0.5433 | val_R: 0.5609\n",
      "[epoch 03] AWL inverse-var weights: {'success_cls': 1.0, 'days_to_state_change': 1.0, 'recommend_category': 1.0, 'goal_eval': 1.0, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 03 | Train Loss: 4.9881 | Val Loss: 4.7183\n",
      "  - success_cls          loss: 0.5256 | val_loss: 0.5331 raw_loss: 0.5256 | raw_val: 0.5331 | train_acc: 0.7448 | val_acc: 0.7406 | train_F1: 0.7238 | val_F1: 0.7306 | train_P: 0.7333 | val_P: 0.7292 | train_R: 0.7204 | val_R: 0.7353\n",
      "  - risk_level           loss: 1.0003 | val_loss: 0.9999 raw_loss: 1.0003 | raw_val: 0.9999 | train_acc: 0.4884 | val_acc: 0.4853 | train_F1: 0.4763 | val_F1: 0.4569 | train_P: 0.4876 | val_P: 0.5048 | train_R: 0.4884 | val_R: 0.4847\n",
      "  - days_to_state_change loss: 0.3671 | val_loss: 0.2253 raw_loss: 0.3671 | raw_val: 0.2253 | train_acc: 0.8591 | val_acc: 0.9180 | train_F1: 0.8589 | val_F1: 0.9166 | train_P: 0.8615 | val_P: 0.9170 | train_R: 0.8594 | val_R: 0.9174\n",
      "  - recommend_category   loss: 1.9467 | val_loss: 1.8463 raw_loss: 1.9467 | raw_val: 1.8463 | train_acc: 0.2793 | val_acc: 0.3083 | train_F1: 0.2333 | val_F1: 0.2564 | train_P: 0.2445 | val_P: 0.2895 | train_R: 0.2792 | val_R: 0.3082\n",
      "  - goal_eval            loss: 0.2373 | val_loss: 0.1965 raw_loss: 0.2373 | raw_val: 0.1965 | train_acc: 0.9117 | val_acc: 0.9301 | train_F1: 0.8948 | val_F1: 0.9169 | train_P: 0.8971 | val_P: 0.9236 | train_R: 0.8948 | val_R: 0.9127\n",
      "  - stretch_potential_cls loss: 0.9111 | val_loss: 0.9172 raw_loss: 0.9111 | raw_val: 0.9172 | train_acc: 0.5677 | val_acc: 0.5604 | train_F1: 0.5388 | val_F1: 0.5159 | train_P: 0.5512 | val_P: 0.5679 | train_R: 0.5471 | val_R: 0.5298\n",
      "[epoch 04] AWL inverse-var weights: {'success_cls': 1.0, 'days_to_state_change': 1.0, 'recommend_category': 1.0, 'goal_eval': 1.0, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 04 | Train Loss: 4.8180 | Val Loss: 4.5894\n",
      "  - success_cls          loss: 0.5238 | val_loss: 0.5399 raw_loss: 0.5238 | raw_val: 0.5399 | train_acc: 0.7416 | val_acc: 0.7252 | train_F1: 0.7253 | val_F1: 0.6959 | train_P: 0.7281 | val_P: 0.7150 | train_R: 0.7253 | val_R: 0.6914\n",
      "  - risk_level           loss: 0.9921 | val_loss: 0.9874 raw_loss: 0.9921 | raw_val: 0.9874 | train_acc: 0.4975 | val_acc: 0.4908 | train_F1: 0.4809 | val_F1: 0.4843 | train_P: 0.4990 | val_P: 0.4874 | train_R: 0.4976 | val_R: 0.4906\n",
      "  - days_to_state_change loss: 0.3341 | val_loss: 0.2221 raw_loss: 0.3341 | raw_val: 0.2221 | train_acc: 0.8767 | val_acc: 0.9200 | train_F1: 0.8763 | val_F1: 0.9193 | train_P: 0.8789 | val_P: 0.9209 | train_R: 0.8768 | val_R: 0.9197\n",
      "  - recommend_category   loss: 1.8452 | val_loss: 1.7548 raw_loss: 1.8452 | raw_val: 1.7548 | train_acc: 0.3207 | val_acc: 0.3575 | train_F1: 0.2822 | val_F1: 0.3244 | train_P: 0.2957 | val_P: 0.3598 | train_R: 0.3208 | val_R: 0.3575\n",
      "  - goal_eval            loss: 0.2138 | val_loss: 0.1733 raw_loss: 0.2138 | raw_val: 0.1733 | train_acc: 0.9200 | val_acc: 0.9357 | train_F1: 0.9039 | val_F1: 0.9236 | train_P: 0.9052 | val_P: 0.9325 | train_R: 0.9046 | val_R: 0.9169\n",
      "  - stretch_potential_cls loss: 0.9090 | val_loss: 0.9119 raw_loss: 0.9090 | raw_val: 0.9119 | train_acc: 0.5712 | val_acc: 0.5680 | train_F1: 0.5404 | val_F1: 0.5532 | train_P: 0.5569 | val_P: 0.5570 | train_R: 0.5489 | val_R: 0.5547\n",
      "[epoch 05] AWL inverse-var weights: {'success_cls': 1.237683892250061, 'days_to_state_change': 1.2603341341018677, 'recommend_category': 0.8311271071434021, 'goal_eval': 1.2549396753311157, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 05 | Train Loss: 4.4157 | Val Loss: 4.1952\n",
      "  - success_cls          loss: 0.5505 | val_loss: 0.5525 raw_loss: 0.5505 | raw_val: 0.5525 | train_acc: 0.7452 | val_acc: 0.7402 | train_F1: 0.7317 | val_F1: 0.7222 | train_P: 0.7322 | val_P: 0.7270 | train_R: 0.7336 | val_R: 0.7206\n",
      "  - risk_level           loss: 0.4235 | val_loss: 0.4196 raw_loss: 0.4235 | raw_val: 0.4196 | train_acc: 0.4979 | val_acc: 0.5016 | train_F1: 0.4776 | val_F1: 0.4683 | train_P: 0.4973 | val_P: 0.4888 | train_R: 0.4980 | val_R: 0.5016\n",
      "  - days_to_state_change loss: 0.6495 | val_loss: 0.5399 raw_loss: 0.6495 | raw_val: 0.5399 | train_acc: 0.8861 | val_acc: 0.9119 | train_F1: 0.8862 | val_F1: 0.9106 | train_P: 0.8877 | val_P: 0.9134 | train_R: 0.8862 | val_R: 0.9110\n",
      "  - recommend_category   loss: 1.9212 | val_loss: 1.8483 raw_loss: 1.9212 | raw_val: 1.8483 | train_acc: 0.3367 | val_acc: 0.3753 | train_F1: 0.3002 | val_F1: 0.3511 | train_P: 0.3146 | val_P: 0.3731 | train_R: 0.3367 | val_R: 0.3752\n",
      "  - goal_eval            loss: 0.4891 | val_loss: 0.4523 raw_loss: 0.4891 | raw_val: 0.4523 | train_acc: 0.9292 | val_acc: 0.9396 | train_F1: 0.9150 | val_F1: 0.9287 | train_P: 0.9162 | val_P: 0.9368 | train_R: 0.9158 | val_R: 0.9221\n",
      "  - stretch_potential_cls loss: 0.3819 | val_loss: 0.3825 raw_loss: 0.3819 | raw_val: 0.3825 | train_acc: 0.5716 | val_acc: 0.5669 | train_F1: 0.5441 | val_F1: 0.5313 | train_P: 0.5561 | val_P: 0.5349 | train_R: 0.5521 | val_R: 0.5522\n",
      "[epoch 06] AWL inverse-var weights: {'success_cls': 1.425865650177002, 'days_to_state_change': 1.3802870512008667, 'recommend_category': 0.7375487685203552, 'goal_eval': 1.4812930822372437, 'risk_level': '(not in AWL; LR-ramped)', 'stretch_potential_cls': '(not in AWL; LR-ramped)'}\n",
      "Epoch 06 | Train Loss: 4.3391 | Val Loss: 4.2059\n",
      "  - success_cls          loss: 0.5361 | val_loss: 0.5373 raw_loss: 0.5361 | raw_val: 0.5373 | train_acc: 0.7599 | val_acc: 0.7553 | train_F1: 0.7486 | val_F1: 0.7473 | train_P: 0.7482 | val_P: 0.7457 | train_R: 0.7535 | val_R: 0.7547\n",
      "  - risk_level           loss: 0.4160 | val_loss: 0.4024 raw_loss: 0.4160 | raw_val: 0.4024 | train_acc: 0.5121 | val_acc: 0.5239 | train_F1: 0.4881 | val_F1: 0.5007 | train_P: 0.5098 | val_P: 0.5203 | train_R: 0.5121 | val_R: 0.5236\n",
      "  - days_to_state_change loss: 0.6020 | val_loss: 0.5455 raw_loss: 0.6020 | raw_val: 0.5455 | train_acc: 0.8819 | val_acc: 0.9102 | train_F1: 0.8816 | val_F1: 0.9091 | train_P: 0.8833 | val_P: 0.9124 | train_R: 0.8821 | val_R: 0.9094\n",
      "  - recommend_category   loss: 1.9385 | val_loss: 1.8985 raw_loss: 1.9385 | raw_val: 1.8985 | train_acc: 0.3205 | val_acc: 0.3332 | train_F1: 0.2908 | val_F1: 0.3013 | train_P: 0.3020 | val_P: 0.3165 | train_R: 0.3206 | val_R: 0.3331\n",
      "  - goal_eval            loss: 0.4727 | val_loss: 0.4567 raw_loss: 0.4727 | raw_val: 0.4567 | train_acc: 0.9240 | val_acc: 0.9316 | train_F1: 0.9094 | val_F1: 0.9175 | train_P: 0.9111 | val_P: 0.9150 | train_R: 0.9094 | val_R: 0.9217\n",
      "  - stretch_potential_cls loss: 0.3737 | val_loss: 0.3654 raw_loss: 0.3737 | raw_val: 0.3654 | train_acc: 0.5865 | val_acc: 0.5990 | train_F1: 0.5580 | val_F1: 0.5786 | train_P: 0.5713 | val_P: 0.6102 | train_R: 0.5670 | val_R: 0.5802\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 165\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_cat, x_cont, y_dict \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    164\u001b[0m     n_batches_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 165\u001b[0m     x_cat, x_cont \u001b[38;5;241m=\u001b[39m \u001b[43mx_cat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, x_cont\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    166\u001b[0m     y_dict \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m y_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    168\u001b[0m     x_cont \u001b[38;5;241m=\u001b[39m feature_dropout(x_cont, drop_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== CONFIG ====\n",
    "num_epochs     = 50\n",
    "warmup_epochs  = 5        \n",
    "patience       = 10\n",
    "focal_gamma = 1.5\n",
    "FOCUS_CFG = {\n",
    "    \"stretch_potential_cls\": {\"max\": 12.0, \"ramp\": 3},  # ดันแรงกว่านิดหน่อย\n",
    "    \"risk_level\":            {\"max\": 12.0, \"ramp\": 3},  # ค่อยๆ ไล่ (ถ้ายังนิ่งค่อยเพิ่ม max เป็น 8)\n",
    "}\n",
    "\n",
    "def get_focus_scale(head: str, epoch: int) -> float:\n",
    "    # ช่วง warmup ไม่บูสต์\n",
    "    if epoch < warmup_epochs:\n",
    "        return 1.0\n",
    "    cfg = FOCUS_CFG.get(head)\n",
    "    if cfg is None:  # หัวอื่นๆ ไม่บูสต์\n",
    "        return 1.0\n",
    "    t = min(1.0, (epoch - warmup_epochs) / max(1, cfg[\"ramp\"]))\n",
    "    return 1.0 + (cfg[\"max\"] - 1.0) * t\n",
    "# ==== Loss dicts ====\n",
    "loss_fn_warmup = {\n",
    "    \"success_cls\":            nn.CrossEntropyLoss(),\n",
    "    # \"success_rate_cls\":       nn.CrossEntropyLoss(label_smoothing=0.1),  \n",
    "    \"risk_level\":             nn.CrossEntropyLoss(),  \n",
    "    \"days_to_state_change\":   nn.CrossEntropyLoss(),\n",
    "    \"recommend_category\":     nn.CrossEntropyLoss(),\n",
    "    \"goal_eval\":              nn.CrossEntropyLoss(),\n",
    "    # \"shortfall_severity_cls\": nn.CrossEntropyLoss(),\n",
    "    \"stretch_potential_cls\":  nn.CrossEntropyLoss(),\n",
    "}\n",
    "\n",
    "# ช่วง post: คมขึ้นเล็กน้อย และมี class weight สำหรับสองหัวสำคัญ (สมมติ cw มีอยู่แล้ว)\n",
    "loss_fn_post = {\n",
    "    \"success_cls\":            nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    # \"success_rate_cls\":       nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    \"risk_level\":             FocalLoss(gamma=2.0, weight=w_risk),\n",
    "    \"days_to_state_change\":   nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    \"recommend_category\":     nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    \"goal_eval\":              nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "    # \"shortfall_severity_cls\": FocalLoss(gamma=gamma_per_class, weight=w_sf, reduction='mean'),\n",
    "    \"stretch_potential_cls\":  FocalLoss(gamma=2.0, weight=w_stretch),\n",
    "}\n",
    "\n",
    "head_names = sorted(list(set(list(loss_fn_warmup.keys()) + list(loss_fn_post.keys()))))\n",
    "\n",
    "# ==== History ====\n",
    "history = {\n",
    "    \"lr_step\": [],\n",
    "    \"lr_epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": {k: [] for k in head_names},\n",
    "    \"val_acc\":   {k: [] for k in head_names},\n",
    "    \"train_f1\":  {k: [] for k in head_names},\n",
    "    \"val_f1\":    {k: [] for k in head_names},\n",
    "    \"train_precision\": {k: [] for k in head_names},\n",
    "    \"val_precision\":   {k: [] for k in head_names},\n",
    "    \"train_recall\":    {k: [] for k in head_names},\n",
    "    \"val_recall\":      {k: [] for k in head_names},\n",
    "    \"train_precision_by_class\": {k: [] for k in head_names},\n",
    "    \"val_precision_by_class\":   {k: [] for k in head_names},\n",
    "    \"train_recall_by_class\":    {k: [] for k in head_names},\n",
    "    \"val_recall_by_class\":      {k: [] for k in head_names},\n",
    "    \"train_loss_by_head\":      {k: [] for k in head_names},\n",
    "    \"val_loss_by_head\":        {k: [] for k in head_names},\n",
    "    \"raw_train_loss_by_head\":  {k: [] for k in head_names},\n",
    "    \"raw_val_loss_by_head\":    {k: [] for k in head_names},\n",
    "}\n",
    "\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     [\n",
    "#         {\"params\": model.parameters(), \"weight_decay\": 7.5e-4, \"lr\": 1e-3},\n",
    "#         {\"params\": awl.parameters(),   \"weight_decay\": 0.0,    \"lr\": 1e-3},\n",
    "#     ]\n",
    "# )\n",
    "base_lr = 1e-3\n",
    "wd      = 7.5e-4\n",
    "\n",
    "stretch_params = [p for n,p in model.named_parameters()\n",
    "                  if 'stretch_potential_cls' in n and p.requires_grad]\n",
    "risk_params    = [p for n,p in model.named_parameters()\n",
    "                  if 'risk_level' in n and p.requires_grad]\n",
    "other_params   = [p for n,p in model.named_parameters()\n",
    "                  if ('stretch_potential_cls' not in n)\n",
    "                  and ('risk_level' not in n)\n",
    "                  and p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": other_params,   \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"backbone\"},\n",
    "    {\"params\": stretch_params, \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"stretch\"},\n",
    "    {\"params\": risk_params,    \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"risk\"},\n",
    "    {\"params\": awl.parameters(),\"lr\": base_lr, \"weight_decay\": 0.0, \"name\": \"awl\"},\n",
    "])\n",
    "\n",
    "# ติดค่า base_lr/last_scale เริ่มต้นให้ทุก group\n",
    "for pg in optimizer.param_groups:\n",
    "    pg[\"base_lr\"]    = pg[\"lr\"]\n",
    "    pg[\"last_scale\"] = 1.0\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     cnt = torch.bincount(y_train_dict['shortfall_severity_cls'].cpu(), minlength=4).float()\n",
    "#     pi_sf = (cnt / cnt.sum()).clamp_min(1e-12)\n",
    "# log_prior_sf = pi_sf.log().to(device)\n",
    "\n",
    "# def tau_schedule(epoch, num_epochs, tau_max=0.5, warm_frac=0.3):\n",
    "#     t = min(1.0, epoch/(warm_frac*num_epochs))\n",
    "#     return tau_max * t\n",
    "def ramp_scale(epoch: int, max_scale: float, ramp_epochs: int):\n",
    "    # ค่อย ๆ ไต่จาก 1.0 -> max_scale ภายใน ramp_epochs\n",
    "    t = min(1.0, epoch / max(1.0, float(ramp_epochs)))\n",
    "    return 1.0 + (max_scale - 1.0) * t\n",
    "\n",
    "# ค่าเริ่มต้น (จูนได้)\n",
    "STRETCH_MAX  = 12.0   # 6–10\n",
    "STRETCH_RAMP = 6     # 3–8\n",
    "RISK_MAX     = 10.0   # 3–6\n",
    "RISK_RAMP    = 6\n",
    "\n",
    "# =============== TRAINING LOOP ===============\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    loss_fn_dict = loss_fn_warmup if epoch < warmup_epochs else loss_fn_post\n",
    "    \n",
    "        # --- boost lr เฉพาะพารามิเตอร์ของหัว (ต้น epoch) ---\n",
    "    stretch_scale = ramp_scale(epoch, STRETCH_MAX, STRETCH_RAMP)\n",
    "    risk_scale    = ramp_scale(epoch, RISK_MAX,    RISK_RAMP)\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        name = pg.get(\"name\", \"\")\n",
    "        if name == \"stretch\":\n",
    "            pg[\"lr\"] = pg[\"base_lr\"] * stretch_scale\n",
    "            pg[\"last_scale\"] = stretch_scale\n",
    "        elif name == \"risk\":\n",
    "            pg[\"lr\"] = pg[\"base_lr\"] * risk_scale\n",
    "            pg[\"last_scale\"] = risk_scale\n",
    "        else:\n",
    "            pg[\"lr\"] = pg[\"base_lr\"]\n",
    "            pg[\"last_scale\"] = 1.0\n",
    "\n",
    "\n",
    "    # ========= TRAIN =========\n",
    "    model.train()\n",
    "    total_losses_by_head_train = {k: 0.0 for k in loss_fn_dict}\n",
    "    raw_losses_by_head_train   = {k: 0.0 for k in loss_fn_dict}\n",
    "    train_acc = {k: [] for k in loss_fn_dict}\n",
    "    train_f1  = {k: [] for k in loss_fn_dict}\n",
    "    train_cache_y = {k: [] for k in loss_fn_dict}\n",
    "    train_cache_p = {k: [] for k in loss_fn_dict}\n",
    "    n_batches_train = 0\n",
    "\n",
    "    # ใช้ oversample เฉพาะช่วง warmup\n",
    "    # train_loader = train_loader_os if (epoch < warmup_epochs) else train_loader_plain\n",
    "    train_loader = train_loader_plain\n",
    "    for x_cat, x_cont, y_dict in train_loader:\n",
    "        n_batches_train += 1\n",
    "        x_cat, x_cont = x_cat.to(device), x_cont.to(device)\n",
    "        y_dict = {k: v.to(device) for k, v in y_dict.items()}\n",
    "\n",
    "        x_cont = feature_dropout(x_cont, drop_prob=0.05)\n",
    "\n",
    "        preds = model(x_cont, x_cat)\n",
    "\n",
    "        raw_losses = {}\n",
    "        for key, pred in preds.items():\n",
    "            target = y_dict[key]\n",
    "\n",
    "            if key in ORDINAL_HEADS:  # CORN\n",
    "                # ---- LOSS ----\n",
    "                if pred.dim() == 3:  # (B, k, C-1)\n",
    "                    loss_value = _tabm_corn_loss(pred, target)\n",
    "                else:                 # (B, C-1)\n",
    "                    loss_value = corn_loss(pred, target)\n",
    "                # ---- PRED ----\n",
    "                pred_class = _ensemble_corn_predict(pred)\n",
    "\n",
    "            elif key in SORD_HEADS:   # SORD\n",
    "                # ---- LOSS ----\n",
    "                if pred.dim() == 3:   # (B, k, C)\n",
    "                    loss_value = _tabm_sord_loss(pred, target)\n",
    "                else:                 # (B, C)\n",
    "                    loss_value = sord_loss(pred, target)\n",
    "                # ---- PRED ----\n",
    "                pred_class = _ensemble_argmax(pred)\n",
    "\n",
    "            else:                     # ปกติ (CrossEntropy)\n",
    "                # ---- LOSS ----\n",
    "                if pred.dim() == 3:   # (B, k, C)\n",
    "                    loss_value = _tabm_ce_loss(pred, target)\n",
    "                else:                 # (B, C)\n",
    "                    loss_value = loss_fn_dict[key](pred, target.long())\n",
    "                # ---- PRED ----\n",
    "                pred_class = _ensemble_argmax(pred)\n",
    "\n",
    "            raw_losses[key] = loss_value\n",
    "\n",
    "            # --- logging (เดิมของคุณ) ---\n",
    "            raw_losses_by_head_train[key]   += loss_value.item()\n",
    "            total_losses_by_head_train[key] += loss_value.item()\n",
    "            tgt = target.detach().cpu()\n",
    "            prd = pred_class.detach().cpu()\n",
    "            train_acc[key].append(accuracy_score(tgt, prd))\n",
    "            train_f1[key].append(f1_score(tgt, prd, average=\"macro\", zero_division=0))\n",
    "            train_cache_y[key].append(tgt.numpy().ravel())\n",
    "            train_cache_p[key].append(prd.numpy().ravel())\n",
    "\n",
    "        # active_heads = [h for h in HEADS_ORDER if h in raw_losses]\n",
    "        # if epoch < warmup_epochs:\n",
    "        #     loss = torch.stack([raw_losses[h] for h in active_heads]).sum()\n",
    "        # else:\n",
    "        #     loss = awl(*[raw_losses[h] for h in active_heads])\n",
    "        # scaled_losses = [\n",
    "        #     raw_losses[h] * get_focus_scale(h, epoch)\n",
    "        #     for h in active_heads\n",
    "        # ]\n",
    "\n",
    "        # if epoch < warmup_epochs:\n",
    "        #     loss = torch.stack(scaled_losses).sum()\n",
    "        # else:\n",
    "        #     loss = awl(*scaled_losses)\n",
    "        \n",
    "        awl_losses   = [raw_losses[h] * get_focus_scale(h, epoch) for h in OTHERS_ORDER if h in raw_losses]\n",
    "        prior_losses = [raw_losses[h] * get_focus_scale(h, epoch) for h in PRIOR_HEADS  if h in raw_losses]\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            loss = torch.stack(awl_losses + prior_losses).sum()\n",
    "        else:\n",
    "            loss = awl(*awl_losses) + (torch.stack(prior_losses).sum() if len(prior_losses) else 0.0)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # --- สรุป train (เดิมของคุณ) ---\n",
    "    for k in total_losses_by_head_train:\n",
    "        total_losses_by_head_train[k] /= max(1, n_batches_train)\n",
    "        raw_losses_by_head_train[k]   /= max(1, n_batches_train)\n",
    "    total_loss_train = sum(total_losses_by_head_train.values())\n",
    "\n",
    "    # macro precision/recall per head (เดิมของคุณ)\n",
    "    for k in loss_fn_dict:\n",
    "        if len(train_cache_y[k]):\n",
    "            y_all = np.concatenate(train_cache_y[k])\n",
    "            p_all = np.concatenate(train_cache_p[k])\n",
    "            tr_prec = precision_score(y_all, p_all, average=\"macro\", zero_division=0)\n",
    "            tr_rec  = recall_score(y_all,   p_all, average=\"macro\", zero_division=0)\n",
    "            history[\"train_precision\"][k].append(float(tr_prec))\n",
    "            history[\"train_recall\"][k].append(float(tr_rec))\n",
    "            labels = np.arange(num_classes_map[k]) if k in num_classes_map else None\n",
    "            tr_prec_cls = precision_score(y_all, p_all, average=None, labels=labels, zero_division=0)\n",
    "            tr_rec_cls  = recall_score(y_all,   p_all, average=None, labels=labels, zero_division=0)\n",
    "            history[\"train_precision_by_class\"][k].append(tr_prec_cls.tolist())\n",
    "            history[\"train_recall_by_class\"][k].append(tr_rec_cls.tolist())\n",
    "        else:\n",
    "            history[\"train_precision\"][k].append(np.nan)\n",
    "            history[\"train_recall\"][k].append(np.nan)\n",
    "            history[\"train_precision_by_class\"][k].append(None)\n",
    "            history[\"train_recall_by_class\"][k].append(None)\n",
    "\n",
    "    # ========= VALIDATION =========\n",
    "    model.eval()\n",
    "    total_losses_by_head_val = {k: 0.0 for k in loss_fn_dict}\n",
    "    raw_losses_by_head_val   = {k: 0.0 for k in loss_fn_dict}\n",
    "    val_acc = {k: [] for k in loss_fn_dict}\n",
    "    val_f1  = {k: [] for k in loss_fn_dict}\n",
    "    val_cache_y = {k: [] for k in loss_fn_dict}\n",
    "    val_cache_p = {k: [] for k in loss_fn_dict}\n",
    "    n_batches_val = 0\n",
    "\n",
    "    # (NEW) เก็บ logits/targets ของ shortfall เพื่อจะเอาไป calibrate ได้ภายหลัง\n",
    "    sf_logits_all, sf_targets_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, y_dict in val_loader:\n",
    "            n_batches_val += 1\n",
    "            x_cat, x_cont = x_cat.to(device), x_cont.to(device)\n",
    "            y_dict = {k: v.to(device) for k, v in y_dict.items()}\n",
    "\n",
    "            preds = model(x_cont, x_cat)\n",
    "            for key, pred in preds.items():\n",
    "                target = y_dict[key]\n",
    "\n",
    "                if key in ORDINAL_HEADS:  # CORN\n",
    "                    # ---- LOSS ----\n",
    "                    if pred.dim() == 3:  # (B, k, C-1)\n",
    "                        val_loss_value = _tabm_corn_loss(pred, target)\n",
    "                    else:                 # (B, C-1)\n",
    "                        val_loss_value = corn_loss(pred, target)\n",
    "                    # ---- PRED ----\n",
    "                    pred_class = _ensemble_corn_predict(pred)\n",
    "\n",
    "                elif key in SORD_HEADS:   # SORD\n",
    "                    # ---- LOSS ----\n",
    "                    if pred.dim() == 3:   # (B, k, C)\n",
    "                        val_loss_value = _tabm_sord_loss(pred, target)\n",
    "                    else:                 # (B, C)\n",
    "                        val_loss_value = sord_loss(pred, target)\n",
    "                    # ---- PRED ----\n",
    "                    pred_class = _ensemble_argmax(pred)\n",
    "\n",
    "                else:                     # ปกติ (CrossEntropy)\n",
    "                    # ---- LOSS ----\n",
    "                    if pred.dim() == 3:   # (B, k, C)\n",
    "                        val_loss_value = _tabm_ce_loss(pred, target)\n",
    "                    else:                 # (B, C)\n",
    "                        val_loss_value = loss_fn_dict[key](pred, target.long())\n",
    "                    # ---- PRED ----\n",
    "                    pred_class = _ensemble_argmax(pred)\n",
    "\n",
    "                raw_losses_by_head_val[key]   += val_loss_value.item()\n",
    "                total_losses_by_head_val[key] += val_loss_value.item()\n",
    "\n",
    "                tgt = target.cpu()\n",
    "                prd = pred_class.cpu()\n",
    "                val_acc[key].append(accuracy_score(tgt, prd))\n",
    "                val_f1[key].append(f1_score(tgt, prd, average=\"macro\", zero_division=0))\n",
    "                val_cache_y[key].append(tgt.numpy().ravel())\n",
    "                val_cache_p[key].append(prd.numpy().ravel())\n",
    "\n",
    "    for k in total_losses_by_head_val:\n",
    "        total_losses_by_head_val[k] /= max(1, n_batches_val)\n",
    "        raw_losses_by_head_val[k]   /= max(1, n_batches_val)\n",
    "    total_loss_val = sum(total_losses_by_head_val.values())\n",
    "\n",
    "    for k in loss_fn_dict:\n",
    "        if len(val_cache_y[k]):\n",
    "            y_all = np.concatenate(val_cache_y[k])\n",
    "            p_all = np.concatenate(val_cache_p[k])\n",
    "            va_prec = precision_score(y_all, p_all, average=\"macro\", zero_division=0)\n",
    "            va_rec  = recall_score(y_all,   p_all, average=\"macro\", zero_division=0)\n",
    "            history[\"val_precision\"][k].append(float(va_prec))\n",
    "            history[\"val_recall\"][k].append(float(va_rec))\n",
    "            labels = np.arange(num_classes_map[k]) if k in num_classes_map else None\n",
    "            va_prec_cls = precision_score(y_all, p_all, average=None, labels=labels, zero_division=0)\n",
    "            va_rec_cls  = recall_score(y_all,   p_all, average=None, labels=labels, zero_division=0)\n",
    "            history[\"val_precision_by_class\"][k].append(va_prec_cls.tolist())\n",
    "            history[\"val_recall_by_class\"][k].append(va_rec_cls.tolist())\n",
    "        else:\n",
    "            history[\"val_precision\"][k].append(np.nan)\n",
    "            history[\"val_recall\"][k].append(np.nan)\n",
    "            history[\"val_precision_by_class\"][k].append(None)\n",
    "            history[\"val_recall_by_class\"][k].append(None)\n",
    "\n",
    "    # --- Scheduler (ของเดิม) ---\n",
    "    focus_heads = [\"stretch_potential_cls\", \"risk_level\"]\n",
    "    if all(h in total_losses_by_head_val for h in focus_heads):\n",
    "        focus_val = sum(total_losses_by_head_val[h] for h in focus_heads)\n",
    "    else:\n",
    "        focus_val = total_loss_val\n",
    "    scheduler.step(focus_val)\n",
    "        \n",
    "    # ให้ ReduceLROnPlateau คุม \"ฐาน\" แล้วอัปเดต base_lr สำหรับรอบหน้า\n",
    "    for pg in optimizer.param_groups:\n",
    "        # lr ตอนนี้ = (base_lr หลัง scheduler) * scale ของ epoch นี้\n",
    "        # แปลงกลับไปเป็น base_lr เพื่อใช้คูณ scale ใหม่ใน epoch หน้า\n",
    "        pg[\"base_lr\"] = pg[\"lr\"] / pg.get(\"last_scale\", 1.0)\n",
    "\n",
    "\n",
    "    # --- log lr/ history/ print (ของเดิม) ---\n",
    "    last_epoch_train_steps = n_batches_train\n",
    "    if last_epoch_train_steps > 0 and len(history[\"lr_step\"]) >= last_epoch_train_steps:\n",
    "        history[\"lr_epoch\"].append(float(np.mean(history[\"lr_step\"][-last_epoch_train_steps:])))\n",
    "    else:\n",
    "        history[\"lr_epoch\"].append(float(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    history[\"train_loss\"].append(total_loss_train)\n",
    "    history[\"val_loss\"].append(total_loss_val)\n",
    "    for k in head_names:\n",
    "        history[\"train_loss_by_head\"][k].append(total_losses_by_head_train.get(k, np.nan))\n",
    "        history[\"val_loss_by_head\"][k].append(total_losses_by_head_val.get(k, np.nan))\n",
    "        history[\"raw_train_loss_by_head\"][k].append(raw_losses_by_head_train.get(k, np.nan))\n",
    "        history[\"raw_val_loss_by_head\"][k].append(raw_losses_by_head_val.get(k, np.nan))\n",
    "        history[\"train_acc\"][k].append(float(np.mean(train_acc.get(k, []))) if len(train_acc.get(k, []))>0 else np.nan)\n",
    "        history[\"val_acc\"][k].append(float(np.mean(val_acc.get(k, []))) if len(val_acc.get(k, []))>0 else np.nan)\n",
    "        history[\"train_f1\"][k].append(float(np.mean(train_f1.get(k, []))) if len(train_f1.get(k, []))>0 else np.nan)\n",
    "        history[\"val_f1\"][k].append(float(np.mean(val_f1.get(k, []))) if len(val_f1.get(k, []))>0 else np.nan)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     inv_sigma2 = 1.0 / (awl.params**2 + 1e-8)\n",
    "    #     awl_weights = {h: float(inv_sigma2[i].item()) for i, h in enumerate(HEADS_ORDER)}\n",
    "    #     print(f\"[epoch {epoch:02d}] AWL inverse-var weights: {awl_weights}\")\n",
    "    with torch.no_grad():\n",
    "        inv_sigma2 = (1.0 / (awl.params**2 + 1e-8)).view(-1)\n",
    "        K = min(len(inv_sigma2), len(OTHERS_ORDER))\n",
    "        awl_weights = {h: float(inv_sigma2[i].item()) for i, h in enumerate(OTHERS_ORDER[:K])}\n",
    "        for h in PRIOR_HEADS:\n",
    "            awl_weights[h] = \"(not in AWL; LR-ramped)\"\n",
    "        print(f\"[epoch {epoch:02d}] AWL inverse-var weights: {awl_weights}\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {total_loss_train:.4f} | Val Loss: {total_loss_val:.4f}\")\n",
    "    for k in loss_fn_dict:\n",
    "        ta = np.mean(train_acc[k]) if len(train_acc[k]) else float('nan')\n",
    "        va = np.mean(val_acc[k])   if len(val_acc[k])   else float('nan')\n",
    "        tf = np.mean(train_f1[k])  if len(train_f1[k])  else float('nan')\n",
    "        vf = np.mean(val_f1[k])    if len(val_f1[k])    else float('nan')\n",
    "        tp = history[\"train_precision\"][k][-1]\n",
    "        vp = history[\"val_precision\"][k][-1]\n",
    "        tr = history[\"train_recall\"][k][-1]\n",
    "        vr = history[\"val_recall\"][k][-1]\n",
    "        print(\n",
    "            f\"  - {k:20s} loss: {total_losses_by_head_train[k]:.4f} | \"\n",
    "            f\"val_loss: {total_losses_by_head_val[k]:.4f} \"\n",
    "            f\"raw_loss: {raw_losses_by_head_train[k]:.4f} | raw_val: {raw_losses_by_head_val[k]:.4f} \"\n",
    "            f\"| train_acc: {ta:.4f} | val_acc: {va:.4f} \"\n",
    "            f\"| train_F1: {tf:.4f} | val_F1: {vf:.4f} \"\n",
    "            f\"| train_P: {tp:.4f} | val_P: {vp:.4f} | train_R: {tr:.4f} | val_R: {vr:.4f}\"\n",
    "        )\n",
    "\n",
    "    # print(\"shortfall val precision by class:\", history[\"val_precision_by_class\"][\"stretch_potential_cls\"][-1])\n",
    "    # print(\"shortfall val recall by class:\",    history[\"val_recall_by_class\"][\"stretch_potential_cls\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Total Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "for head in history[\"train_acc\"]:\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_acc\"][head], label=f\"{head} Train Acc\")\n",
    "    plt.plot(history[\"val_acc\"][head], label=f\"{head} Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{head} Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "for head in history[\"train_f1\"]:\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_f1\"][head], label=f\"{head} Train F1\")\n",
    "    plt.plot(history[\"val_f1\"][head], label=f\"{head} Val F1\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{head} Macro F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1-score\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
